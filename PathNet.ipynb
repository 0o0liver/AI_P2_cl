{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PathNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qhDlkNIBpuY0",
        "6sjChmPcIe7b",
        "-BlDasoOZ-dr",
        "NnM9oAWHqJcT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dchu1/AI_P2_cl/blob/master/PathNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvNezhZno5qi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_tasks = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhDlkNIBpuY0",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeD2BTRwpw0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.nn import Module\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sjChmPcIe7b",
        "colab_type": "text"
      },
      "source": [
        "# Constructing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTOALc9Zusuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_dataset(d, rotation):\n",
        "  result = torch.FloatTensor(d.size(0), 784)\n",
        "  tensor = transforms.ToTensor()\n",
        "\n",
        "  for i in range(d.size(0)):\n",
        "    img = Image.fromarray(d[i].numpy(), mode=\"L\")\n",
        "    result[i] = tensor(img.rotate(rotation)).view(784)\n",
        "  return result\n",
        "\n",
        "mnist_path = \"mnist.npz\"\n",
        "if not os.path.exists(os.path.join(\"/content\", mnist_path)):\n",
        "  subprocess.call(\"wget https://s3.amazonaws.com/img-datasets/mnist.npz\", shell=True)\n",
        "\n",
        "f = np.load(mnist_path)\n",
        "x_tr = torch.from_numpy(f[\"x_train\"])\n",
        "y_tr = torch.from_numpy(f[\"y_train\"]).long()\n",
        "x_te = torch.from_numpy(f[\"x_test\"])\n",
        "y_te = torch.from_numpy(f[\"y_test\"]).long()\n",
        "f.close()\n",
        "\n",
        "# Rotate Dataset\n",
        "tasks_tr = []\n",
        "tasks_te = []\n",
        "mnist_rot_path = \"mnist_rotations.pt\"\n",
        "if not os.path.exists(os.path.join(\"/content\", mnist_rot_path)):\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    for t in range(n_tasks):\n",
        "      min_rot = 1.0 * t / n_tasks * (180.0 - 0.0) + 0.0\n",
        "      max_rot = 1.0 * (t + 1) / n_tasks * (180.0 - 0.0) + 0.0\n",
        "      rot = random.random() * (max_rot - min_rot) + min_rot\n",
        "\n",
        "      tasks_tr.append([rot, rotate_dataset(x_tr, rot), y_tr])\n",
        "      tasks_te.append([rot, rotate_dataset(x_te, rot), y_te])\n",
        "\n",
        "    torch.save([tasks_tr, tasks_te], 'mnist_rotations.pt')\n",
        "else:\n",
        "    tasks_tr, tasks_te = torch.load('/content/mnist_rotations.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-BlDasoOZ-dr"
      },
      "source": [
        "# Genotype Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft0BF107TuLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "class Genotype():\n",
        "\n",
        "    def __init__(self, L, M, N):\n",
        "        self.genotype = np.random.randint(0, M, (L,N))\n",
        "        self.L = L\n",
        "        self.M = M\n",
        "        self.N = N\n",
        "\n",
        "    def apply_mutation(self, i, j):\n",
        "        gene = self.genotype[i][j] + random.randint(-2, 2)\n",
        "        if gene < 0:\n",
        "            gene += self.M\n",
        "        elif gene > self.M - 1:\n",
        "            gene -= self.M\n",
        "        self.genotype[i][j] = gene\n",
        "\n",
        "    def mutate(self):\n",
        "        for i in range(self.L):\n",
        "            for j in range(self.N):\n",
        "                if random.random() < 1.0 / (self.L * self.N):\n",
        "                    self.apply_mutation(i, j)\n",
        "\n",
        "    def return_genotype(self):\n",
        "        return self.genotype\n",
        "\n",
        "    def overwrite(self, genotype):\n",
        "        self.genotype = copy.deepcopy(genotype)\n",
        "\n",
        "\n",
        "class Genetic():\n",
        "\n",
        "    def __init__(self, L, M, N, pop): \n",
        "        \"\"\"L: layers, M: units in each layer, N: number of active units, pop: number of gene\"\"\"\n",
        "        self.genotypes = [Genotype(L, M, N) for _ in range(pop)]\n",
        "        self.pop = pop\n",
        "        self.control_fixed = random.sample(self.genotypes,1)[0]\n",
        "\n",
        "    def return_all_genotypes(self):\n",
        "        genotypes = [gene.return_genotype() for gene in self.genotypes]\n",
        "        return genotypes\n",
        "\n",
        "    def return_control(self):\n",
        "        return self.control_fixed\n",
        "\n",
        "    def return_control_genotype(self):\n",
        "        return self.control_fixed.return_genotype()\n",
        "\n",
        "    def sample(self):\n",
        "        return random.sample(self.genotypes, 2)\n",
        "\n",
        "    def overwrite(self, genotypes, fitnesses):\n",
        "        win = genotypes[fitnesses.index(max(fitnesses))]\n",
        "        lose = genotypes[fitnesses.index(min(fitnesses))]\n",
        "        genotype = win.return_genotype()\n",
        "        lose.overwrite(genotype)\n",
        "        lose.mutate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DIOkaIcca0N",
        "colab_type": "text"
      },
      "source": [
        "# PathNet Model 2 Layer Conv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIW5UOOtizUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import gradcheck\n",
        "\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, L, M, N, num_neurons, lr, use_cuda):\n",
        "        super(Net, self).__init__()\n",
        "        self.L = L\n",
        "        self.M = M\n",
        "        self.N = N\n",
        "        self.lr = lr\n",
        "        self.num_neurons = num_neurons\n",
        "        self.use_cuda = use_cuda\n",
        "        self.final_layers = []\n",
        "        #self.init(None, None)\n",
        "\n",
        "    def init(self, best_path, task):\n",
        "        nochange_modules = [[]] * self.L\n",
        "        if len(best_path) == 0:\n",
        "            nochange_modules = [[None] * self.M] * self.L\n",
        "        else:\n",
        "            for i in range(len(best_path)):\n",
        "                nochange_modules = np.concatenate((nochange_modules,best_path[i]), axis=1)\n",
        "        num_neurons = self.num_neurons\n",
        "        module_num = [self.M] * self.L\n",
        "  \n",
        "        # Construct our network. Don't touch modules that are frozen (nochange_modules)\n",
        "        self.fc1 = []\n",
        "        self.fc2 = []\n",
        "        # self.fc3 = []\n",
        "\n",
        "        for i in range(module_num[0]):\n",
        "            if not i in nochange_modules[0]:\n",
        "                \"\"\"All parameters should be declared as member variable, so I think this is the simplest way to do so\"\"\"\n",
        "                exec(\"self.m1\" + str(i) + \" = nn.Conv2d(1, 32, 3, 1)\")\n",
        "            exec(\"self.fc1.append(self.m1\" + str(i) + \")\")\n",
        "\n",
        "        for i in range(module_num[1]):\n",
        "            if not i in nochange_modules[1]:\n",
        "                exec(\"self.m2\" + str(i) + \" = nn.Conv2d(32, 64, 3, 1)\")\n",
        "            exec(\"self.fc2.append(self.m2\" + str(i) + \")\")\n",
        "\n",
        "        # for i in range(module_num[2]):\n",
        "        #     if not i in best_path[2]:\n",
        "        #         #exec(\"self.m3\" + str(i) + \" = nn.Linear(\" + str(neuron_num) + \", 10)\")\n",
        "        #         exec(\"self.m3\" + str(i) + \" = nn.Linear(\" + str(neuron_num) + \",\" + str(neuron_num) + \")\")\n",
        "        #     exec(\"self.fc3.append(self.m3\" + str(i) + \")\")\n",
        "\n",
        "\n",
        "        if task != None:\n",
        "            exec(\"self.final_layer\" + str(task) + \" = nn.Linear(9216, 10)\")\n",
        "            exec(\"self.final_layers.append(\" + \"self.final_layer\" + str(task) + \")\")\n",
        "\n",
        "        # Get our trainable params for the optimizer\n",
        "        trainable_params = []\n",
        "        params_set = [self.fc1, self.fc2]\n",
        "        for path, params in zip(nochange_modules, params_set):\n",
        "            #print(\"Fixing Layer parameters\")\n",
        "            for i, param in enumerate(params):\n",
        "                if  i in path:\n",
        "                    param.requires_grad = False\n",
        "                    #print(\"Fixing Module {} parameters\".format(i))\n",
        "                else:\n",
        "                    p = {'params': param.parameters()}\n",
        "                    trainable_params.append(p)\n",
        "                    \n",
        "        p = {'params': self.final_layers[-1].parameters()}\n",
        "        trainable_params.append(p)\n",
        "        self.optimizer = optim.SGD(trainable_params, lr=self.lr)\n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "    def forward(self, x, path, last):\n",
        "        # flatten the 28*28 images into 1d array for linear layers\n",
        "        #x = x.view(-1, 28*28)\n",
        "        # reshape 1d array into 28*28 images\n",
        "        #x = x.view(16,1,28,28)\n",
        "\n",
        "        #M = self.M\n",
        "        y = F.relu(self.fc1[path[0][0]](x))\n",
        "        for j in range(1,len(path[0])):\n",
        "            y += F.relu(self.fc1[path[0][j]](x))\n",
        "        x = y\n",
        "        y = F.relu(self.fc2[path[1][0]](x))\n",
        "        for j in range(1,len(path[0])):\n",
        "            y += F.relu(self.fc2[path[1][j]](x))\n",
        "        x = y\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        # y = F.relu(self.fc3[path[2][0]](x))\n",
        "        # for j in range(1,self.N):\n",
        "        #     y += F.relu(self.fc3[path[2][j]](x))\n",
        "        # x = y\n",
        "        \n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.final_layers[last](x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "    def train_model(self, train_loader, path, num_batch):\n",
        "        self.train()\n",
        "        fitness = float(0)\n",
        "        train_len = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            if self.use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            self.optimizer.zero_grad()\n",
        "            #x = data.view(16,1,28,28)\n",
        "            output = self(data.view(data.size(0),1,28,28), path, -1)\n",
        "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "            correct = pred.eq(target.data).cpu().sum()\n",
        "            fitness += correct\n",
        "            train_len += len(target.data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            #print(\"Batch:\", batch_idx, \"Acc:\", str(correct.item()) + \"/\" + str(len(target.data)))\n",
        "            self.optimizer.step()\n",
        "            if not batch_idx < num_batch -1:\n",
        "                break\n",
        "        #set_trace()\n",
        "        fitness = fitness / train_len\n",
        "        return fitness\n",
        "\n",
        "    def test_model(self, test_loader, path, last):\n",
        "        self.eval()\n",
        "\n",
        "        # For now we will throw out the path given to us and just run on the\n",
        "        # entire network.\n",
        "        # path = [np.arange(self.M)] * 2\n",
        "\n",
        "        fitness = float(0)\n",
        "        train_len = 0\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            if self.use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            self.optimizer.zero_grad()\n",
        "            #x = data.view(1,1,28,28)\n",
        "            output = self(data.view(1000,1,28,28), path, last)\n",
        "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "            fitness += pred.eq(target.data).cpu().sum()\n",
        "            train_len += len(target.data)\n",
        "\n",
        "        fitness = fitness / train_len\n",
        "        return fitness\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr0OQ5RWImXF",
        "colab_type": "text"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI9uv-uKBiGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_pathnet(model, gene, train_loader, num_batch, best_fitness, best_path, gen):\n",
        "    pathways = gene.sample()\n",
        "    fitnesses = []\n",
        "    train_data = [(data, target) for (data,target) in train_loader]\n",
        "    for pathway in pathways:\n",
        "        path = pathway.return_genotype()\n",
        "        fitness = model.train_model(train_data, path, num_batch)\n",
        "        fitnesses.append(fitness)\n",
        "    #print(\"Generation {} : Fitnesses = {} vs {}\".format(gen, fitnesses[0], fitnesses[1]))\n",
        "\n",
        "    gene.overwrite(pathways, fitnesses)\n",
        "    genes = gene.return_all_genotypes()\n",
        "    #visualizer.show(genes, vis_color)\n",
        "    if max(fitnesses) > best_fitness:\n",
        "        best_fitness = max(fitnesses)\n",
        "        best_path = pathways[fitnesses.index(max(fitnesses))].return_genotype()\n",
        "    #print(\"Generation {} : Best Fitness = {}\".format(gen, best_fitness.item()))\n",
        "    return best_fitness, best_path, max(fitnesses)\n",
        "\n",
        "def train_control(model, gene, train_loader, gen):        \n",
        "    path = gene.return_control_genotype()\n",
        "    train_data = [(data, target) for (data,target) in train_loader]\n",
        "    fitness = model.train_model(train_data, path, args.num_batch)\n",
        "    #print(\"Generation {} : Fitness = {}\".format(gen, fitness))\n",
        "    genes = [gene.return_control_genotype()] * args.pop\n",
        "    #visualizer.show(genes, 'm')\n",
        "    return fitness\n",
        "\n",
        "def evaluate_on_tasks(model, tasks_trained, test_loaders, best_paths):\n",
        "    print(\"Evaluating on task test sets\")\n",
        "    test_acc = []\n",
        "    for k in range(n_tasks):\n",
        "        i = k if k <= tasks_trained else -1\n",
        "        fitness = model.test_model(test_loaders[k], best_paths[i], i)\n",
        "        print(\"Test Accuracy on Task Set {}: {}\".format(k, fitness))\n",
        "        test_acc.append(fitness.item())\n",
        "    print(\"Average Test Accuracy: {}\".format(np.mean(test_acc)))\n",
        "    return test_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTwMhvtOweQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    # Training settings\n",
        "    L = 2\n",
        "    M = 10\n",
        "    N = 4\n",
        "    pop = 64\n",
        "    batch_size = 16\n",
        "    batch_limit = 150\n",
        "    lr = 0.01\n",
        "    num_neurons = 20\n",
        "    generation_limit = 50\n",
        "    control = False\n",
        "    fine_tune = False\n",
        "    use_cuda = True\n",
        "    seed = 0\n",
        "\n",
        "    cuda = use_cuda and torch.cuda.is_available()\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "    model = Net(L, M, N, num_neurons, lr, cuda)\n",
        "    #module_num = [args.M] * args.L\n",
        "\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    if not os.path.isdir('./result'):\n",
        "        os.makedirs(\"./result\")\n",
        "\n",
        "    if os.path.exists('./result/result_mnist.pickle'):\n",
        "        f = open('./result/result_mnist.pickle','rb')\n",
        "        result = pickle.load(f)\n",
        "        f.close()\n",
        "    else:\n",
        "        result = []\n",
        "\n",
        "    test_loaders = []\n",
        "    for i in range(n_tasks):\n",
        "        test_loaders.append(data_utils.DataLoader(data_utils.TensorDataset(tasks_te[i][1], tasks_te[i][2]), batch_size=1000, shuffle = False))\n",
        "\n",
        "    best_paths = []\n",
        "    best_path = [[None] * N] * L\n",
        "    total_test_acc = []\n",
        "    tasks_fitness = []\n",
        "    gene = Genetic(L, M, N, pop)\n",
        "\n",
        "    # Initialize (or reinitialise) our model\n",
        "    #model.init(best_paths, 0)\n",
        "    # First we will evaluate before doing any training\n",
        "    #total_test_acc.append(evaluate_on_tasks(model, i, test_loaders))\n",
        "\n",
        "    # Then train across tasks\n",
        "    for i in range(n_tasks):\n",
        "        gen = 0\n",
        "        best_fitness = 0.0\n",
        "        task_fitness = []\n",
        "\n",
        "        # If not control, generate a new gene which controls the permutations of\n",
        "        # pathways\n",
        "        if not control:\n",
        "          gene = Genetic(L, M, N, pop)\n",
        "\n",
        "        # Initialize (or reinitialise) our model\n",
        "        model.init(best_paths, i+1)\n",
        "        \n",
        "        # Load our training data\n",
        "        train = data_utils.TensorDataset(tasks_tr[i][1], tasks_tr[i][2])\n",
        "        train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle = True)\n",
        "        print(\"Training Task {} started...\".format(i))\n",
        "        \n",
        "        # Begin our training tournament\n",
        "        for gen in range(generation_limit):\n",
        "            if not control:\n",
        "                best_fitness, best_path, max_fitness = train_pathnet(model, gene, train_loader, batch_limit, best_fitness, best_path, gen)\n",
        "                task_fitness.append(max_fitness)\n",
        "            else: ##control experiment\n",
        "                fitness = train_control(model, gene, train_loader, gen)\n",
        "                task_fitness.append(fitness)\n",
        "\n",
        "        # Store our best fitness and path\n",
        "        tasks_fitness.append(task_fitness)\n",
        "        best_paths.append(best_path)\n",
        "        #print(\"Best Paths:\", best_paths)\n",
        "        # append the new best path to the existing best path (if its not the first)\n",
        "        # iteration. Otherwise the new best path is the best path\n",
        "        # if i == 0:\n",
        "        #     set_trace()\n",
        "        #     best_path = new_best_path\n",
        "        #     print(best_path)\n",
        "        # else:\n",
        "        #     best_path = np.concatenate((best_path, new_best_path), axis=1)\n",
        "        #     print(best_path)\n",
        "\n",
        "        # Evaluate on our test sets\n",
        "        test_acc = evaluate_on_tasks(model, i, test_loaders, best_paths)\n",
        "        total_test_acc.append(test_acc)\n",
        "\n",
        "        # test_acc = []\n",
        "        # for k in range(n_tasks):\n",
        "        #     fitness = model.test_model(tests[k], best_path, k if k <= i else -1)\n",
        "        #     #fitness = model.test_model(tests[k], best_path, -1)\n",
        "        #     print(\"After Training Task {}: Test Accuracy on Task {}: {}\".format(i, k, fitness))\n",
        "        #     test_acc.append(fitness.item())\n",
        "        # total_test_acc.append(test_acc)\n",
        "        # print(\"Average Test Accuracy After Training Task {}: {}\".format(i,np.mean(test_acc)))\n",
        "        print(\"Task {} done.\".format(i))\n",
        "\n",
        "    average_acc = np.mean(total_test_acc[n_tasks-1])\n",
        "    print(\"Accuracy:\", average_acc)\n",
        "    print(\"Confusion matrix:\")\n",
        "    print('\\n'.join([','.join([str(item) for item in row]) for row in total_test_acc]))\n",
        "    # print(total_test_acc)\n",
        "    # for i in range(len(total_test_acc)):\n",
        "    #     print(\"{}(mean={})\".format(total_test_acc[i], np.mean(total_test_acc[i])))\n",
        "\n",
        "    # Save our results\n",
        "    if control:\n",
        "        if fine_tune:\n",
        "            result.append(('fine_tune', total_test_acc))\n",
        "        else:\n",
        "            result.append(('control', total_test_acc))\n",
        "    else:\n",
        "        result.append(('confusion_matrix', total_test_acc))\n",
        "    f = open('./result/result_mnist.pickle', 'wb')\n",
        "    pickle.dump(result, f)\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN_7QFO2fNB2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c6cc71f4-0fa5-45db-f0d7-f7bccd537f52"
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Task 0 started...\n",
            "Evaluating on task test sets\n",
            "Test Accuracy on Task Set 0: 0.9754999876022339\n",
            "Test Accuracy on Task Set 1: 0.9574000239372253\n",
            "Test Accuracy on Task Set 2: 0.8403000235557556\n",
            "Test Accuracy on Task Set 3: 0.6802999973297119\n",
            "Test Accuracy on Task Set 4: 0.5446000099182129\n",
            "Test Accuracy on Task Set 5: 0.3659000098705292\n",
            "Test Accuracy on Task Set 6: 0.31439998745918274\n",
            "Test Accuracy on Task Set 7: 0.22589999437332153\n",
            "Test Accuracy on Task Set 8: 0.1720000058412552\n",
            "Test Accuracy on Task Set 9: 0.15960000455379486\n",
            "Test Accuracy on Task Set 10: 0.1542000025510788\n",
            "Test Accuracy on Task Set 11: 0.14740000665187836\n",
            "Test Accuracy on Task Set 12: 0.15279999375343323\n",
            "Test Accuracy on Task Set 13: 0.17389999330043793\n",
            "Test Accuracy on Task Set 14: 0.21950000524520874\n",
            "Test Accuracy on Task Set 15: 0.22679999470710754\n",
            "Test Accuracy on Task Set 16: 0.2881999909877777\n",
            "Test Accuracy on Task Set 17: 0.3237999975681305\n",
            "Test Accuracy on Task Set 18: 0.3237000107765198\n",
            "Test Accuracy on Task Set 19: 0.3160000145435333\n",
            "Average Test Accuracy: 0.37811000272631645\n",
            "Task 0 done.\n",
            "Training Task 1 started...\n",
            "Evaluating on task test sets\n",
            "Test Accuracy on Task Set 0: 0.9754999876022339\n",
            "Test Accuracy on Task Set 1: 0.9767000079154968\n",
            "Test Accuracy on Task Set 2: 0.9340999722480774\n",
            "Test Accuracy on Task Set 3: 0.8407999873161316\n",
            "Test Accuracy on Task Set 4: 0.73089998960495\n",
            "Test Accuracy on Task Set 5: 0.5544999837875366\n",
            "Test Accuracy on Task Set 6: 0.477400004863739\n",
            "Test Accuracy on Task Set 7: 0.31439998745918274\n",
            "Test Accuracy on Task Set 8: 0.21359999477863312\n",
            "Test Accuracy on Task Set 9: 0.18469999730587006\n",
            "Test Accuracy on Task Set 10: 0.17339999973773956\n",
            "Test Accuracy on Task Set 11: 0.14710000157356262\n",
            "Test Accuracy on Task Set 12: 0.14350000023841858\n",
            "Test Accuracy on Task Set 13: 0.14489999413490295\n",
            "Test Accuracy on Task Set 14: 0.17329999804496765\n",
            "Test Accuracy on Task Set 15: 0.17919999361038208\n",
            "Test Accuracy on Task Set 16: 0.22630000114440918\n",
            "Test Accuracy on Task Set 17: 0.26460000872612\n",
            "Test Accuracy on Task Set 18: 0.2903999984264374\n",
            "Test Accuracy on Task Set 19: 0.28360000252723694\n",
            "Average Test Accuracy: 0.4114449955523014\n",
            "Task 1 done.\n",
            "Training Task 2 started...\n",
            "Evaluating on task test sets\n",
            "Test Accuracy on Task Set 0: 0.9754999876022339\n",
            "Test Accuracy on Task Set 1: 0.9767000079154968\n",
            "Test Accuracy on Task Set 2: 0.9765999913215637\n",
            "Test Accuracy on Task Set 3: 0.9646999835968018\n",
            "Test Accuracy on Task Set 4: 0.9405999779701233\n",
            "Test Accuracy on Task Set 5: 0.857699990272522\n",
            "Test Accuracy on Task Set 6: 0.7878000140190125\n",
            "Test Accuracy on Task Set 7: 0.5450000166893005\n",
            "Test Accuracy on Task Set 8: 0.26969999074935913\n",
            "Test Accuracy on Task Set 9: 0.17679999768733978\n",
            "Test Accuracy on Task Set 10: 0.14980000257492065\n",
            "Test Accuracy on Task Set 11: 0.11739999800920486\n",
            "Test Accuracy on Task Set 12: 0.10790000110864639\n",
            "Test Accuracy on Task Set 13: 0.1096000000834465\n",
            "Test Accuracy on Task Set 14: 0.13030000030994415\n",
            "Test Accuracy on Task Set 15: 0.1347000002861023\n",
            "Test Accuracy on Task Set 16: 0.1761000007390976\n",
            "Test Accuracy on Task Set 17: 0.24040000140666962\n",
            "Test Accuracy on Task Set 18: 0.3000999987125397\n",
            "Test Accuracy on Task Set 19: 0.29980000853538513\n",
            "Average Test Accuracy: 0.4618599984794855\n",
            "Task 2 done.\n",
            "Training Task 3 started...\n",
            "Evaluating on task test sets\n",
            "Test Accuracy on Task Set 0: 0.9754999876022339\n",
            "Test Accuracy on Task Set 1: 0.9767000079154968\n",
            "Test Accuracy on Task Set 2: 0.9765999913215637\n",
            "Test Accuracy on Task Set 3: 0.9779000282287598\n",
            "Test Accuracy on Task Set 4: 0.9693999886512756\n",
            "Test Accuracy on Task Set 5: 0.930899977684021\n",
            "Test Accuracy on Task Set 6: 0.88919997215271\n",
            "Test Accuracy on Task Set 7: 0.7261000275611877\n",
            "Test Accuracy on Task Set 8: 0.44110000133514404\n",
            "Test Accuracy on Task Set 9: 0.2980000078678131\n",
            "Test Accuracy on Task Set 10: 0.2483000010251999\n",
            "Test Accuracy on Task Set 11: 0.17190000414848328\n",
            "Test Accuracy on Task Set 12: 0.15530000627040863\n",
            "Test Accuracy on Task Set 13: 0.1339000016450882\n",
            "Test Accuracy on Task Set 14: 0.12710000574588776\n",
            "Test Accuracy on Task Set 15: 0.121799997985363\n",
            "Test Accuracy on Task Set 16: 0.12970000505447388\n",
            "Test Accuracy on Task Set 17: 0.15600000321865082\n",
            "Test Accuracy on Task Set 18: 0.2061000019311905\n",
            "Test Accuracy on Task Set 19: 0.22040000557899475\n",
            "Average Test Accuracy: 0.4915950011461973\n",
            "Task 3 done.\n",
            "Training Task 4 started...\n",
            "Evaluating on task test sets\n",
            "Test Accuracy on Task Set 0: 0.9754999876022339\n",
            "Test Accuracy on Task Set 1: 0.9767000079154968\n",
            "Test Accuracy on Task Set 2: 0.9765999913215637\n",
            "Test Accuracy on Task Set 3: 0.9779000282287598\n",
            "Test Accuracy on Task Set 4: 0.9761999845504761\n",
            "Test Accuracy on Task Set 5: 0.9657999873161316\n",
            "Test Accuracy on Task Set 6: 0.9527999758720398\n",
            "Test Accuracy on Task Set 7: 0.8632000088691711\n",
            "Test Accuracy on Task Set 8: 0.6090999841690063\n",
            "Test Accuracy on Task Set 9: 0.39980000257492065\n",
            "Test Accuracy on Task Set 10: 0.3319999873638153\n",
            "Test Accuracy on Task Set 11: 0.2037999927997589\n",
            "Test Accuracy on Task Set 12: 0.17870000004768372\n",
            "Test Accuracy on Task Set 13: 0.13609999418258667\n",
            "Test Accuracy on Task Set 14: 0.11729999631643295\n",
            "Test Accuracy on Task Set 15: 0.11110000312328339\n",
            "Test Accuracy on Task Set 16: 0.11460000276565552\n",
            "Test Accuracy on Task Set 17: 0.1324000060558319\n",
            "Test Accuracy on Task Set 18: 0.1754000037908554\n",
            "Test Accuracy on Task Set 19: 0.19619999825954437\n",
            "Average Test Accuracy: 0.5185599971562624\n",
            "Task 4 done.\n",
            "Training Task 5 started...\n",
            "Evaluating on task test sets\n",
            "Test Accuracy on Task Set 0: 0.9754999876022339\n",
            "Test Accuracy on Task Set 1: 0.9767000079154968\n",
            "Test Accuracy on Task Set 2: 0.9765999913215637\n",
            "Test Accuracy on Task Set 3: 0.9779000282287598\n",
            "Test Accuracy on Task Set 4: 0.9761999845504761\n",
            "Test Accuracy on Task Set 5: 0.9739999771118164\n",
            "Test Accuracy on Task Set 6: 0.9718999862670898\n",
            "Test Accuracy on Task Set 7: 0.9380999803543091\n",
            "Test Accuracy on Task Set 8: 0.7972000241279602\n",
            "Test Accuracy on Task Set 9: 0.6152999997138977\n",
            "Test Accuracy on Task Set 10: 0.519599974155426\n",
            "Test Accuracy on Task Set 11: 0.3325999975204468\n",
            "Test Accuracy on Task Set 12: 0.2662999927997589\n",
            "Test Accuracy on Task Set 13: 0.17260000109672546\n",
            "Test Accuracy on Task Set 14: 0.14059999585151672\n",
            "Test Accuracy on Task Set 15: 0.1331000030040741\n",
            "Test Accuracy on Task Set 16: 0.12070000171661377\n",
            "Test Accuracy on Task Set 17: 0.12479999661445618\n",
            "Test Accuracy on Task Set 18: 0.14790000021457672\n",
            "Test Accuracy on Task Set 19: 0.15610000491142273\n",
            "Average Test Accuracy: 0.564684996753931\n",
            "Task 5 done.\n",
            "Training Task 6 started...\n",
            "Evaluating on task test sets\n",
            "Test Accuracy on Task Set 0: 0.9754999876022339\n",
            "Test Accuracy on Task Set 1: 0.9767000079154968\n",
            "Test Accuracy on Task Set 2: 0.9765999913215637\n",
            "Test Accuracy on Task Set 3: 0.9779000282287598\n",
            "Test Accuracy on Task Set 4: 0.9761999845504761\n",
            "Test Accuracy on Task Set 5: 0.9739999771118164\n",
            "Test Accuracy on Task Set 6: 0.9739999771118164\n",
            "Test Accuracy on Task Set 7: 0.9503999948501587\n",
            "Test Accuracy on Task Set 8: 0.8242999911308289\n",
            "Test Accuracy on Task Set 9: 0.6514999866485596\n",
            "Test Accuracy on Task Set 10: 0.5461999773979187\n",
            "Test Accuracy on Task Set 11: 0.3424000144004822\n",
            "Test Accuracy on Task Set 12: 0.28279998898506165\n",
            "Test Accuracy on Task Set 13: 0.19820000231266022\n",
            "Test Accuracy on Task Set 14: 0.16179999709129333\n",
            "Test Accuracy on Task Set 15: 0.15530000627040863\n",
            "Test Accuracy on Task Set 16: 0.1412000060081482\n",
            "Test Accuracy on Task Set 17: 0.1396999955177307\n",
            "Test Accuracy on Task Set 18: 0.15569999814033508\n",
            "Test Accuracy on Task Set 19: 0.16290000081062317\n",
            "Average Test Accuracy: 0.5771649956703186\n",
            "Task 6 done.\n",
            "Training Task 7 started...\n",
            "Evaluating on task test sets\n",
            "Test Accuracy on Task Set 0: 0.9754999876022339\n",
            "Test Accuracy on Task Set 1: 0.9767000079154968\n",
            "Test Accuracy on Task Set 2: 0.9765999913215637\n",
            "Test Accuracy on Task Set 3: 0.9779000282287598\n",
            "Test Accuracy on Task Set 4: 0.9761999845504761\n",
            "Test Accuracy on Task Set 5: 0.9739999771118164\n",
            "Test Accuracy on Task Set 6: 0.9739999771118164\n",
            "Test Accuracy on Task Set 7: 0.9569000005722046\n",
            "Test Accuracy on Task Set 8: 0.8806999921798706\n",
            "Test Accuracy on Task Set 9: 0.7595999836921692\n",
            "Test Accuracy on Task Set 10: 0.6712999939918518\n",
            "Test Accuracy on Task Set 11: 0.4440999925136566\n",
            "Test Accuracy on Task Set 12: 0.35350000858306885\n",
            "Test Accuracy on Task Set 13: 0.23100000619888306\n",
            "Test Accuracy on Task Set 14: 0.16110000014305115\n",
            "Test Accuracy on Task Set 15: 0.1559000015258789\n",
            "Test Accuracy on Task Set 16: 0.14339999854564667\n",
            "Test Accuracy on Task Set 17: 0.147599995136261\n",
            "Test Accuracy on Task Set 18: 0.16850000619888306\n",
            "Test Accuracy on Task Set 19: 0.15620000660419464\n",
            "Average Test Accuracy: 0.6030349969863892\n",
            "Task 7 done.\n",
            "Training Task 8 started...\n",
            "Evaluating on task test sets\n",
            "Test Accuracy on Task Set 0: 0.9754999876022339\n",
            "Test Accuracy on Task Set 1: 0.9767000079154968\n",
            "Test Accuracy on Task Set 2: 0.9765999913215637\n",
            "Test Accuracy on Task Set 3: 0.9779000282287598\n",
            "Test Accuracy on Task Set 4: 0.9761999845504761\n",
            "Test Accuracy on Task Set 5: 0.9739999771118164\n",
            "Test Accuracy on Task Set 6: 0.9739999771118164\n",
            "Test Accuracy on Task Set 7: 0.9569000005722046\n",
            "Test Accuracy on Task Set 8: 0.9782999753952026\n",
            "Test Accuracy on Task Set 9: 0.9624999761581421\n",
            "Test Accuracy on Task Set 10: 0.9402999877929688\n",
            "Test Accuracy on Task Set 11: 0.8497999906539917\n",
            "Test Accuracy on Task Set 12: 0.7728999853134155\n",
            "Test Accuracy on Task Set 13: 0.5546000003814697\n",
            "Test Accuracy on Task Set 14: 0.351500004529953\n",
            "Test Accuracy on Task Set 15: 0.3231000006198883\n",
            "Test Accuracy on Task Set 16: 0.20509999990463257\n",
            "Test Accuracy on Task Set 17: 0.14959999918937683\n",
            "Test Accuracy on Task Set 18: 0.14380000531673431\n",
            "Test Accuracy on Task Set 19: 0.1339000016450882\n",
            "Average Test Accuracy: 0.7076599940657615\n",
            "Task 8 done.\n",
            "Training Task 9 started...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnM9oAWHqJcT",
        "colab_type": "text"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCnGbHT1wbVq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
        "parser.add_argument('--mnist', action='store_true', default=True,\n",
        "                    help='open mnist result')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "\n",
        "def subplot(subplot, data_first, data_second, data_third, data_first_control, data_second_control, data_third_control, title):\n",
        "    plt.subplot(subplot)\n",
        "    if args.mnist:\n",
        "        x = np.arange(0,100)\n",
        "    else:\n",
        "        x = np.arange(0,500)\n",
        "    y_first = np.mean(data_first, axis=0)\n",
        "    y_second = np.mean(data_second, axis=0)\n",
        "    y_third = np.mean(data_third, axis=0)\n",
        "    y_first_control = np.mean(data_first_control, axis=0)\n",
        "    y_second_control = np.mean(data_second_control, axis=0)\n",
        "    y_third_control = np.mean(data_third_control, axis=0)\n",
        "    #y_first_err = np.std(data_first, axis=0) / 2.\n",
        "    #y_second_err = np.std(data_second, axis=0) / 2. \n",
        "    #y_third_err = np.std(data_third, axis=0) / 2. \n",
        "    \n",
        "    #plt.fill_between(x, y_first - y_first_err, y_first + y_first_err, color='m', alpha=0.3)\n",
        "    #plt.fill_between(x, y_second - y_second_err, y_second + y_second_err, color='y', alpha=0.3)\n",
        "    #plt.fill_between(x, y_third - y_third_err, y_third + y_third_err, color='c', alpha=0.3)\n",
        "    plt.plot(x, y_first, color='r', label='Task A')\n",
        "    plt.plot(x, y_second, color='g', label='Task B')\n",
        "    plt.plot(x, y_third, color='b', label='Task C')\n",
        "    plt.plot(x, y_first_control, linestyle='dashed', color='r', label='Task A (control')\n",
        "    plt.plot(x, y_second_control, linestyle='dashed', color='g', label='Task B (control)')\n",
        "    plt.plot(x, y_third_control, linestyle='dashed', color='b', label='Task C (control)')\n",
        "    plt.legend(bbox_to_anchor=(0.8, 0.3), loc=2, ncol=1, fontsize=15)\n",
        "    axes = plt.gca()\n",
        "\n",
        "    if args.mnist:\n",
        "        axes.set_xlim([0, 100])\n",
        "        axes.set_ylim([0, 1.2])\n",
        "    else:\n",
        "        axes.set_xlim([0, 500])\n",
        "        axes.set_ylim([0, 0.6])\n",
        "    plt.title(title, fontsize=20, y = 0.9)\n",
        "    plt.ylabel('Accuracy',fontsize=15)\n",
        "    plt.xlabel('Generations',fontsize=15)\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "try: \n",
        "    if args.mnist:\n",
        "        f = open(os.path.join('./result/result_mnist.pickle'), 'rb')\n",
        "        result = pickle.load(f)\n",
        "        f.close()\n",
        "        pathnet_first = [result[1][2]]\n",
        "        pathnet_second = [result[1][3]]\n",
        "        pathnet_third = [result[1][4]]\n",
        "\n",
        "        pathnet_first_control = [result[0][2]]\n",
        "        pathnet_second_control = [result[0][3]]\n",
        "        pathnet_third_control = [result[0][4]]\n",
        "\n",
        "        #for res in result:\n",
        "            #pathnet_first.append(res[2])\n",
        "            #pathnet_second.append(res[3])\n",
        "            #pathnet_third.append(res[4])\n",
        "\n",
        "        subplot('111', pathnet_first, pathnet_second, pathnet_third, pathnet_first_control, pathnet_second_control, pathnet_third_control, 'MNIST')\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        f = open(os.path.join('./result/result_cifar_svhn.pickle'))\n",
        "        result = pickle.load(f)\n",
        "        f.close()\n",
        "\n",
        "        cifar_first = []\n",
        "        cifar_second = []\n",
        "        svhn_first = []\n",
        "        svhn_second = []\n",
        "\n",
        "        for res in result:\n",
        "            if res[0] == 'pathnet_cifar_first':\n",
        "                cifar_first.append(res[2])\n",
        "                svhn_second.append(res[3])\n",
        "            else:\n",
        "                svhn_first.append(res[2])\n",
        "                cifar_second.append(res[3])\n",
        "\n",
        "        subplot('211', cifar_first, cifar_second,'CIFAR-10')\n",
        "        subplot('212', svhn_first, svhn_second,'cSVHN')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "except IOError:\n",
        "    print(\"Result file does not exist\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcT5Bas2sorB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}