{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PathNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qhDlkNIBpuY0",
        "6sjChmPcIe7b",
        "-BlDasoOZ-dr",
        "NnM9oAWHqJcT"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dchu1/AI_P2_cl/blob/master/PathNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhDlkNIBpuY0",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeD2BTRwpw0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch.utils.data as data_utils\n",
        "import math\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import random\n",
        "import copy\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import gradcheck\n",
        "from PIL import Image\n",
        "from torchvision import datasets, transforms\n",
        "from IPython.core.debugger import set_trace\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.nn import Module\n",
        "\n",
        "n_tasks = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sjChmPcIe7b",
        "colab_type": "text"
      },
      "source": [
        "# Constructing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTOALc9Zusuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_dataset(d, rotation):\n",
        "  result = torch.FloatTensor(d.size(0), 784)\n",
        "  tensor = transforms.ToTensor()\n",
        "\n",
        "  for i in range(d.size(0)):\n",
        "    img = Image.fromarray(d[i].numpy(), mode=\"L\")\n",
        "    result[i] = tensor(img.rotate(rotation)).view(784)\n",
        "  return result\n",
        "\n",
        "mnist_path = \"mnist.npz\"\n",
        "if not os.path.exists(os.path.join(\"/content\", mnist_path)):\n",
        "  subprocess.call(\"wget https://s3.amazonaws.com/img-datasets/mnist.npz\", shell=True)\n",
        "\n",
        "f = np.load(mnist_path)\n",
        "x_tr = torch.from_numpy(f[\"x_train\"])\n",
        "y_tr = torch.from_numpy(f[\"y_train\"]).long()\n",
        "x_te = torch.from_numpy(f[\"x_test\"])\n",
        "y_te = torch.from_numpy(f[\"y_test\"]).long()\n",
        "f.close()\n",
        "\n",
        "# Rotate Dataset\n",
        "tasks_tr = []\n",
        "tasks_te = []\n",
        "mnist_rot_path = \"mnist_rotations.pt\"\n",
        "if not os.path.exists(os.path.join(\"/content\", mnist_rot_path)):\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    for t in range(n_tasks):\n",
        "      min_rot = 1.0 * t / n_tasks * (180.0 - 0.0) + 0.0\n",
        "      max_rot = 1.0 * (t + 1) / n_tasks * (180.0 - 0.0) + 0.0\n",
        "      rot = random.random() * (max_rot - min_rot) + min_rot\n",
        "\n",
        "      tasks_tr.append([rot, rotate_dataset(x_tr, rot), y_tr])\n",
        "      tasks_te.append([rot, rotate_dataset(x_te, rot), y_te])\n",
        "\n",
        "    torch.save([tasks_tr, tasks_te], 'mnist_rotations.pt')\n",
        "else:\n",
        "    tasks_tr, tasks_te = torch.load('/content/mnist_rotations.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-BlDasoOZ-dr"
      },
      "source": [
        "# Genotype Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft0BF107TuLS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Genotype():\n",
        "\n",
        "    def __init__(self, L, M, N):\n",
        "        self.genotype = np.random.randint(0, M, (L,N))\n",
        "        self.L = L\n",
        "        self.M = M\n",
        "        self.N = N\n",
        "\n",
        "    def apply_mutation(self, i, j):\n",
        "        gene = self.genotype[i][j] + random.randint(-2, 2)\n",
        "        if gene < 0:\n",
        "            gene += self.M\n",
        "        elif gene > self.M - 1:\n",
        "            gene -= self.M\n",
        "        self.genotype[i][j] = gene\n",
        "\n",
        "    def mutate(self):\n",
        "        for i in range(self.L):\n",
        "            for j in range(self.N):\n",
        "                if random.random() < 1.0 / (self.L * self.N):\n",
        "                    self.apply_mutation(i, j)\n",
        "\n",
        "    def return_genotype(self):\n",
        "        return self.genotype\n",
        "\n",
        "    def overwrite(self, genotype):\n",
        "        self.genotype = copy.deepcopy(genotype)\n",
        "\n",
        "\n",
        "class Genetic():\n",
        "\n",
        "    def __init__(self, L, M, N, pop): \n",
        "        \"\"\"\n",
        "        L: layers, M: units in each layer, N: number of active units, pop: number of gene\n",
        "        \"\"\"\n",
        "        self.genotypes = [Genotype(L, M, N) for _ in range(pop)]\n",
        "        self.pop = pop\n",
        "        self.control_fixed = random.sample(self.genotypes,1)[0]\n",
        "\n",
        "    def return_all_genotypes(self):\n",
        "        genotypes = [gene.return_genotype() for gene in self.genotypes]\n",
        "        return genotypes\n",
        "\n",
        "    def return_control(self):\n",
        "        return self.control_fixed\n",
        "\n",
        "    def return_control_genotype(self):\n",
        "        return self.control_fixed.return_genotype()\n",
        "\n",
        "    def sample(self):\n",
        "        return random.sample(self.genotypes, 2)\n",
        "\n",
        "    def overwrite(self, genotypes, fitnesses):\n",
        "        win = genotypes[fitnesses.index(max(fitnesses))]\n",
        "        lose = genotypes[fitnesses.index(min(fitnesses))]\n",
        "        genotype = win.return_genotype()\n",
        "        lose.overwrite(genotype)\n",
        "        lose.mutate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DIOkaIcca0N",
        "colab_type": "text"
      },
      "source": [
        "# PathNet Model 2 Layer Conv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIW5UOOtizUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self, L, M, N, num_neurons, lr, use_cuda):\n",
        "        super(Net, self).__init__()\n",
        "        self.L = L\n",
        "        self.M = M\n",
        "        self.N = N\n",
        "        self.lr = lr\n",
        "        self.num_neurons = num_neurons\n",
        "        self.use_cuda = use_cuda\n",
        "        self.final_layers = []\n",
        "\n",
        "    def init(self, best_path, task):\n",
        "        # Figure out which modules not to change based on which ones are in the\n",
        "        # best path\n",
        "        nochange_modules = [[]] * self.L\n",
        "        if len(best_path) == 0:\n",
        "            nochange_modules = [[None] * self.M] * self.L\n",
        "        else:\n",
        "            for i in range(len(best_path)):\n",
        "                nochange_modules = np.concatenate((nochange_modules,best_path[i]), axis=1)\n",
        "        num_neurons = self.num_neurons\n",
        "        module_num = [self.M] * self.L\n",
        "  \n",
        "        # Construct our network. Don't touch modules that are frozen (nochange_modules)\n",
        "        self.fc1 = []\n",
        "        self.fc2 = []\n",
        "        \n",
        "        # TODO: Refactor these to use ModuleLists instead\n",
        "        for i in range(module_num[0]):\n",
        "            if not i in nochange_modules[0]:\n",
        "                \"\"\"\n",
        "                All parameters should be declared as member variable, \n",
        "                so I think this is the simplest way to do so\n",
        "                \"\"\"\n",
        "                exec(\"self.m1\" + str(i) + \" = nn.Conv2d(1, 32, 3, 1)\")\n",
        "            exec(\"self.fc1.append(self.m1\" + str(i) + \")\")\n",
        "\n",
        "        for i in range(module_num[1]):\n",
        "            if not i in nochange_modules[1]:\n",
        "                exec(\"self.m2\" + str(i) + \" = nn.Conv2d(32, 64, 3, 1)\")\n",
        "            exec(\"self.fc2.append(self.m2\" + str(i) + \")\")\n",
        "\n",
        "        if task != None:\n",
        "            exec(\"self.final_layer\" + str(task) + \" = nn.Linear(9216, 10)\")\n",
        "            exec(\"self.final_layers.append(\" + \"self.final_layer\" + str(task) + \")\")\n",
        "\n",
        "        # Get our trainable params for the optimizer\n",
        "        trainable_params = []\n",
        "        params_set = [self.fc1, self.fc2]\n",
        "        for path, params in zip(nochange_modules, params_set):\n",
        "            for i, param in enumerate(params):\n",
        "                if  i in path:\n",
        "                    param.requires_grad = False\n",
        "                else:\n",
        "                    p = {'params': param.parameters()}\n",
        "                    trainable_params.append(p)\n",
        "                    \n",
        "        p = {'params': self.final_layers[-1].parameters()}\n",
        "        trainable_params.append(p)\n",
        "        self.optimizer = optim.SGD(trainable_params, lr=self.lr)\n",
        "        if self.use_cuda:\n",
        "            self.cuda()\n",
        "\n",
        "    def forward(self, x, path, last):\n",
        "        y = F.relu(self.fc1[path[0][0]](x))\n",
        "        for j in range(1,len(path[0])):\n",
        "            y += F.relu(self.fc1[path[0][j]](x))\n",
        "        x = y\n",
        "        y = F.relu(self.fc2[path[1][0]](x))\n",
        "        for j in range(1,len(path[0])):\n",
        "            y += F.relu(self.fc2[path[1][j]](x))\n",
        "        x = y\n",
        "        x = F.max_pool2d(x, 2)\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.final_layers[last](x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "    def train_model(self, train_loader, path, num_batch):\n",
        "        self.train()\n",
        "        fitness = float(0)\n",
        "        train_len = 0\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            if self.use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self(data.view(data.size(0),1,28,28), path, -1)\n",
        "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "            correct = pred.eq(target.data).cpu().sum()\n",
        "            fitness += correct\n",
        "            train_len += len(target.data)\n",
        "            loss = F.cross_entropy(output, target)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            if not batch_idx < num_batch -1:\n",
        "                break\n",
        "        fitness = fitness / train_len\n",
        "        return fitness\n",
        "\n",
        "    def test_model(self, test_loader, path, last):\n",
        "        self.eval()\n",
        "\n",
        "        # For now we will throw out the path given to us and just run on the entire network.\n",
        "\n",
        "        fitness = float(0)\n",
        "        train_len = 0\n",
        "        for batch_idx, (data, target) in enumerate(test_loader):\n",
        "            if self.use_cuda:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            data, target = Variable(data), Variable(target)\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self(data.view(1000,1,28,28), path, last)\n",
        "            pred = output.data.max(1)[1] # get the index of the max log-probability\n",
        "            fitness += pred.eq(target.data).cpu().sum()\n",
        "            train_len += len(target.data)\n",
        "\n",
        "        fitness = fitness / train_len\n",
        "        return fitness\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr0OQ5RWImXF",
        "colab_type": "text"
      },
      "source": [
        "# Run Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI9uv-uKBiGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_pathnet(model, gene, train_loader, num_batch, best_fitness, best_path, gen):\n",
        "    pathways = gene.sample()\n",
        "    fitnesses = []\n",
        "    train_data = [(data, target) for (data,target) in train_loader]\n",
        "    for pathway in pathways:\n",
        "        path = pathway.return_genotype()\n",
        "        fitness = model.train_model(train_data, path, num_batch)\n",
        "        fitnesses.append(fitness)\n",
        "\n",
        "    gene.overwrite(pathways, fitnesses)\n",
        "    genes = gene.return_all_genotypes()\n",
        "\n",
        "    if max(fitnesses) > best_fitness:\n",
        "        best_fitness = max(fitnesses)\n",
        "        best_path = pathways[fitnesses.index(max(fitnesses))].return_genotype()\n",
        "    \n",
        "    return best_fitness, best_path, max(fitnesses)\n",
        "\n",
        "def train_control(model, gene, train_loader, batch_limit, gen):        \n",
        "    path = gene.return_control_genotype()\n",
        "    train_data = [(data, target) for (data,target) in train_loader]\n",
        "    fitness = model.train_model(train_data, path, batch_limit)\n",
        "    return fitness\n",
        "\n",
        "def evaluate_on_tasks(model, tasks_trained, test_loaders, best_paths):\n",
        "    print(\"Evaluating on task test sets\")\n",
        "    test_acc = []\n",
        "    for k in range(n_tasks):\n",
        "        i = k if k <= tasks_trained else -1\n",
        "        fitness = model.test_model(test_loaders[k], best_paths[i], i)\n",
        "        if print_messages:\n",
        "            print(\"Test Accuracy on Task Set {}: {}\".format(k, fitness))\n",
        "        test_acc.append(fitness.item())\n",
        "    print(\"Average Test Accuracy: {}\".format(np.mean(test_acc)))\n",
        "    return test_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTwMhvtOweQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_messages = False\n",
        "def main():\n",
        "    # Training settings\n",
        "    L = 2\n",
        "    M = 10\n",
        "    N = 4\n",
        "    pop = 64\n",
        "    batch_size = 16\n",
        "    batch_limit = 150\n",
        "    lr = 0.01\n",
        "    num_neurons = 20\n",
        "    generation_limit = 50\n",
        "    control = False\n",
        "    fine_tune = False\n",
        "    use_cuda = True\n",
        "    seed = 0\n",
        "\n",
        "    cuda = use_cuda and torch.cuda.is_available()\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "    model = Net(L, M, N, num_neurons, lr, cuda)\n",
        "\n",
        "    if cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    if not os.path.isdir('./result'):\n",
        "        os.makedirs(\"./result\")\n",
        "\n",
        "    if os.path.exists('./result/result_mnist.pickle'):\n",
        "        f = open('./result/result_mnist.pickle','rb')\n",
        "        result = pickle.load(f)\n",
        "        f.close()\n",
        "    else:\n",
        "        result = []\n",
        "\n",
        "    test_loaders = []\n",
        "    for i in range(n_tasks):\n",
        "        test_loaders.append(data_utils.DataLoader(data_utils.TensorDataset(tasks_te[i][1], tasks_te[i][2]), batch_size=1000, shuffle = False))\n",
        "\n",
        "    best_paths = []\n",
        "    best_path = [[None] * N] * L\n",
        "    total_test_acc = []\n",
        "    tasks_fitness = []\n",
        "\n",
        "    gene = Genetic(L, M, N, pop)\n",
        "\n",
        "    # Then train across tasks\n",
        "    for i in range(n_tasks):\n",
        "        gen = 0\n",
        "        best_fitness = 0.0\n",
        "        task_fitness = []\n",
        "\n",
        "        # If not control, generate a new gene which controls the permutations of pathways\n",
        "        if not fine_tune:\n",
        "          gene = Genetic(L, M, N, pop)\n",
        "\n",
        "        # Initialize (or reinitialise) our model\n",
        "        model.init(best_paths, i+1)\n",
        "        \n",
        "        # Before we start doing any training, we are going to do an evaluation \n",
        "        # with the initialized weights to get a baseline\n",
        "        if i == 0:\n",
        "            test_acc = evaluate_on_tasks(model, i, test_loaders, [gene.return_control_genotype()]*(i+1))\n",
        "            total_test_acc.append(test_acc)\n",
        "\n",
        "        # Load our training data\n",
        "        train = data_utils.TensorDataset(tasks_tr[i][1], tasks_tr[i][2])\n",
        "        train_loader = data_utils.DataLoader(train, batch_size=batch_size, shuffle = True)\n",
        "        print(\"Training Task {} started...\".format(i))\n",
        "        \n",
        "        # Begin our training tournament\n",
        "        for gen in range(generation_limit):\n",
        "            if not control:\n",
        "                best_fitness, best_path, max_fitness = train_pathnet(model, gene, train_loader, batch_limit, best_fitness, best_path, gen)\n",
        "                task_fitness.append(max_fitness)\n",
        "            else: # control experiment\n",
        "                fitness = train_control(model, gene, train_loader, batch_limit, gen)\n",
        "                task_fitness.append(fitness)\n",
        "\n",
        "        # Store our best fitness and path\n",
        "        tasks_fitness.append(task_fitness)\n",
        "        best_paths.append(best_path)\n",
        "        \n",
        "        # Evaluate on our test sets\n",
        "        if not control:\n",
        "            test_acc = evaluate_on_tasks(model, i, test_loaders, best_paths)\n",
        "        else:\n",
        "            test_acc = evaluate_on_tasks(model, i, test_loaders, [gene.return_control_genotype()]*(i+1))\n",
        "        total_test_acc.append(test_acc)\n",
        "\n",
        "        print(\"Task {} done.\".format(i))\n",
        "\n",
        "    average_acc = np.mean(total_test_acc[n_tasks-1])\n",
        "    print(\"Accuracy:\", average_acc)\n",
        "    print(\"Confusion matrix:\")\n",
        "    print('\\n'.join([','.join([str(item) for item in row]) for row in total_test_acc]))\n",
        "\n",
        "    # Save our results\n",
        "    if control:\n",
        "        if fine_tune:\n",
        "            result.append(('fine_tune', total_test_acc))\n",
        "        else:\n",
        "            result.append(('control', total_test_acc))\n",
        "    else:\n",
        "        result.append(('confusion_matrix', total_test_acc))\n",
        "    f = open('./result/result_mnist.pickle', 'wb')\n",
        "    pickle.dump(result, f)\n",
        "    f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN_7QFO2fNB2",
        "colab_type": "code",
        "outputId": "7e81f9d8-0d05-4b89-fad7-d8eae426b7ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.10320000350475311\n",
            "Training Task 0 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.38537499904632566\n",
            "Task 0 done.\n",
            "Training Task 1 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.430444997921586\n",
            "Task 1 done.\n",
            "Training Task 2 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.46060499772429464\n",
            "Task 2 done.\n",
            "Training Task 3 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.49093500077724456\n",
            "Task 3 done.\n",
            "Training Task 4 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.5480999991297721\n",
            "Task 4 done.\n",
            "Training Task 5 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.5812649987637997\n",
            "Task 5 done.\n",
            "Training Task 6 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.6117099978029727\n",
            "Task 6 done.\n",
            "Training Task 7 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.6248999983072281\n",
            "Task 7 done.\n",
            "Training Task 8 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.6758849956095219\n",
            "Task 8 done.\n",
            "Training Task 9 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.7046499960124493\n",
            "Task 9 done.\n",
            "Training Task 10 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.7731400012969971\n",
            "Task 10 done.\n",
            "Training Task 11 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.8238650031387806\n",
            "Task 11 done.\n",
            "Training Task 12 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.8654100000858307\n",
            "Task 12 done.\n",
            "Training Task 13 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.9023599997162819\n",
            "Task 13 done.\n",
            "Training Task 14 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.9134900018572807\n",
            "Task 14 done.\n",
            "Training Task 15 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.9299000024795532\n",
            "Task 15 done.\n",
            "Training Task 16 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.9591649949550629\n",
            "Task 16 done.\n",
            "Training Task 17 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.9746899962425232\n",
            "Task 17 done.\n",
            "Training Task 18 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.9749349981546402\n",
            "Task 18 done.\n",
            "Training Task 19 started...\n",
            "Evaluating on task test sets\n",
            "Average Test Accuracy: 0.9759700000286102\n",
            "Task 19 done.\n",
            "Accuracy: 0.9749349981546402\n",
            "Confusion matrix:\n",
            "0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311,0.10320000350475311\n",
            "0.9801999926567078,0.9398999810218811,0.8968999981880188,0.7164000272750854,0.5364999771118164,0.38659998774528503,0.3012999892234802,0.26829999685287476,0.20059999823570251,0.18690000474452972,0.15309999883174896,0.13619999587535858,0.14069999754428864,0.1695999950170517,0.20020000636577606,0.22830000519752502,0.2921000123023987,0.3222000002861023,0.32330000400543213,0.32820001244544983\n",
            "0.9801999926567078,0.9785000085830688,0.9753000140190125,0.9312999844551086,0.823199987411499,0.6601999998092651,0.515999972820282,0.4350000023841858,0.27300000190734863,0.2143000066280365,0.16110000014305115,0.1354999989271164,0.1234000027179718,0.11829999834299088,0.1257999986410141,0.13779999315738678,0.19460000097751617,0.25600001215934753,0.28299999237060547,0.2863999903202057\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9602000117301941,0.8902000188827515,0.7623999714851379,0.6128000020980835,0.521399974822998,0.3197999894618988,0.23980000615119934,0.1657000035047531,0.14319999516010284,0.148499995470047,0.15369999408721924,0.16019999980926514,0.16750000417232513,0.19750000536441803,0.2498999983072281,0.2858999967575073,0.29409998655319214\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9539999961853027,0.911899983882904,0.8245000243186951,0.7573000192642212,0.5214999914169312,0.38769999146461487,0.2231999933719635,0.16040000319480896,0.1451999992132187,0.12849999964237213,0.12309999763965607,0.11789999902248383,0.12080000340938568,0.14959999918937683,0.18729999661445618,0.19660000503063202\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9588000178337097,0.9217000007629395,0.8730999827384949,0.6959999799728394,0.5846999883651733,0.36419999599456787,0.24070000648498535,0.18529999256134033,0.16599999368190765,0.16380000114440918,0.164900004863739,0.16910000145435333,0.18240000307559967,0.19900000095367432,0.20679999887943268\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9735999703407288,0.9643999934196472,0.8896999955177307,0.8130000233650208,0.5205000042915344,0.321399986743927,0.22110000252723694,0.16169999539852142,0.14880000054836273,0.1428000032901764,0.14270000159740448,0.14509999752044678,0.1559000015258789,0.16179999709129333\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9693999886512756,0.930400013923645,0.8881999850273132,0.6603999733924866,0.46050000190734863,0.31709998846054077,0.219200000166893,0.18880000710487366,0.1671999990940094,0.1462000012397766,0.14219999313354492,0.1525000035762787,0.1590999960899353\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9621999859809875,0.9307000041007996,0.7736999988555908,0.5497999787330627,0.3747999966144562,0.23729999363422394,0.18050000071525574,0.14880000054836273,0.12229999899864197,0.13019999861717224,0.13770000636577606,0.1404000073671341\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.965499997138977,0.8866999745368958,0.7434999942779541,0.5584999918937683,0.3806999921798706,0.31119999289512634,0.2644999921321869,0.18039999902248383,0.15080000460147858,0.1467999964952469,0.14300000667572021\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.941100001335144,0.8690000176429749,0.7343999743461609,0.519599974155426,0.40059998631477356,0.2992999851703644,0.1696999967098236,0.12880000472068787,0.1315000057220459,0.13439999520778656\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.9768000245094299,0.9628000259399414,0.9147999882698059,0.782800018787384,0.6762999892234802,0.5575000047683716,0.3224000036716461,0.19660000503063202,0.15889999270439148,0.1492999941110611\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.9768000245094299,0.9810000061988831,0.97079998254776,0.9232000112533569,0.8672000169754028,0.7785000205039978,0.5253000259399414,0.2955999970436096,0.21690000593662262,0.17739999294281006\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.9768000245094299,0.9810000061988831,0.9749000072479248,0.9663000106811523,0.9485999941825867,0.9124000072479248,0.724399983882904,0.4641999900341034,0.32829999923706055,0.26669999957084656\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.9768000245094299,0.9810000061988831,0.9749000072479248,0.9765999913215637,0.9747999906539917,0.9664000272750854,0.883899986743927,0.6572999954223633,0.4991999864578247,0.39169999957084656\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.9768000245094299,0.9810000061988831,0.9749000072479248,0.9765999913215637,0.9706000089645386,0.9664000272750854,0.9049000144004822,0.7195000052452087,0.5684999823570251,0.4659999907016754\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.9768000245094299,0.9810000061988831,0.9749000072479248,0.9765999913215637,0.9706000089645386,0.9692999720573425,0.9375,0.8029000163078308,0.667900025844574,0.5759000182151794\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.9768000245094299,0.9810000061988831,0.9749000072479248,0.9765999913215637,0.9706000089645386,0.9692999720573425,0.9732999801635742,0.9333999752998352,0.8622999787330627,0.8004999756813049\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.9768000245094299,0.9810000061988831,0.9749000072479248,0.9765999913215637,0.9706000089645386,0.9692999720573425,0.9732999801635742,0.9775999784469604,0.9718999862670898,0.9571999907493591\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.9768000245094299,0.9810000061988831,0.9749000072479248,0.9765999913215637,0.9706000089645386,0.9692999720573425,0.9732999801635742,0.9775999784469604,0.9750000238418579,0.9589999914169312\n",
            "0.9801999926567078,0.9785000085830688,0.9805999994277954,0.9699000120162964,0.9763000011444092,0.9772999882698059,0.9702000021934509,0.9765999913215637,0.9764999747276306,0.9785000085830688,0.9768000245094299,0.9810000061988831,0.9749000072479248,0.9765999913215637,0.9706000089645386,0.9692999720573425,0.9732999801635742,0.9775999784469604,0.9750000238418579,0.9797000288963318\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}