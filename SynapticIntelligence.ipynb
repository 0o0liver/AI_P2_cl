{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SynapticIntelligence.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x4JqoVPDvOVs"
      ],
      "authorship_tag": "ABX9TyNnypgPM3DXnH9UjcKLBCW1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dchu1/AI_P2_cl/blob/master/SynapticIntelligence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfQFMn10URTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_tasks = 20\n",
        "n_epochs = 3\n",
        "print_messages = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEXzD84NQHfr",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyJtcwJQKwOg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "563b9ec9-8391-43b2-a71d-55a12ee0c336"
      },
      "source": [
        "pip install ax-platform"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ax-platform\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/e5/defa97540bf23447f15d142a644eed9a9d9fd1925cf1e3c4f47a49282ec0/ax_platform-0.1.9-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from ax-platform) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ax-platform) (1.0.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from ax-platform) (1.4.1)\n",
            "Collecting botorch==0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/e4/d696b12a84d505e9592fb6f8458a968b19efc22e30cc517dd2d2817e27e4/botorch-0.2.1-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 32.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from ax-platform) (4.4.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from ax-platform) (2.11.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ax-platform) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ax-platform) (1.18.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->ax-platform) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->ax-platform) (2018.9)\n",
            "Collecting gpytorch>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/5f/ce79e35c1a36deb25a0eac0f67bfe85fb8350eb8e19223950c3d615e5e9a/gpytorch-1.0.1.tar.gz (229kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from botorch==0.2.1->ax-platform) (1.4.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->ax-platform) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly->ax-platform) (1.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->ax-platform) (1.1.1)\n",
            "Building wheels for collected packages: gpytorch\n",
            "  Building wheel for gpytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpytorch: filename=gpytorch-1.0.1-py2.py3-none-any.whl size=390441 sha256=0995ea6764052cca65a0d4ccb506f7a25d1ae71fd843a3803b7f08b917b96390\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/2f/7a/3328e5713d796daeec2ce8ded141d5f3837253fc3c2a5c62e0\n",
            "Successfully built gpytorch\n",
            "Installing collected packages: gpytorch, botorch, ax-platform\n",
            "Successfully installed ax-platform-0.1.9 botorch-0.2.1 gpytorch-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdLhz_0GjWA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.nn import Module\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import torch.utils.data as data_utils\n",
        "\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import os\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4JqoVPDvOVs",
        "colab_type": "text"
      },
      "source": [
        "# Constructing Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrev6f7FvRIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_dataset(d, rotation):\n",
        "  result = torch.FloatTensor(d.size(0), 784)\n",
        "  tensor = transforms.ToTensor()\n",
        "\n",
        "  for i in range(d.size(0)):\n",
        "    img = Image.fromarray(d[i].numpy(), mode=\"L\")\n",
        "    result[i] = tensor(img.rotate(rotation)).view(784)\n",
        "  return result\n",
        "\n",
        "mnist_path = \"mnist.npz\"\n",
        "if not os.path.exists(os.path.join(\"/content\", mnist_path)):\n",
        "  subprocess.call(\"wget https://s3.amazonaws.com/img-datasets/mnist.npz\", shell=True)\n",
        "\n",
        "f = np.load(mnist_path)\n",
        "x_tr = torch.from_numpy(f[\"x_train\"])\n",
        "y_tr = torch.from_numpy(f[\"y_train\"]).long()\n",
        "x_te = torch.from_numpy(f[\"x_test\"])\n",
        "y_te = torch.from_numpy(f[\"y_test\"]).long()\n",
        "f.close()\n",
        "\n",
        "# Rotate Dataset\n",
        "tasks_tr = []\n",
        "tasks_te = []\n",
        "mnist_rot_path = \"mnist_rotations.pt\"\n",
        "if not os.path.exists(os.path.join(\"/content\", mnist_rot_path)):\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    for t in range(n_tasks):\n",
        "      min_rot = 1.0 * t / n_tasks * (180.0 - 0.0) + 0.0\n",
        "      max_rot = 1.0 * (t + 1) / n_tasks * (180.0 - 0.0) + 0.0\n",
        "      rot = random.random() * (max_rot - min_rot) + min_rot\n",
        "\n",
        "      tasks_tr.append([rot, rotate_dataset(x_tr, rot), y_tr])\n",
        "      tasks_te.append([rot, rotate_dataset(x_te, rot), y_te])\n",
        "\n",
        "    torch.save([tasks_tr, tasks_te], 'mnist_rotations.pt')\n",
        "else:\n",
        "    tasks_tr, tasks_te = torch.load('/content/mnist_rotations.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3w5psinhrGb",
        "colab_type": "text"
      },
      "source": [
        "# Defining Synaptic Intelligence Model (Simple)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DhQQzqOhuJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SIModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SIModel, self).__init__()\n",
        "\n",
        "        # SI Hyperparameters\n",
        "        self.si_c = 0.           #-> hyperparam: how strong to weigh SI-loss (\"regularisation strength\")\n",
        "        self.epsilon = 0.1      #-> dampening parameter: bounds 'omega' when squared parameter-change goes to 0\n",
        "    \n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.kaiming_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "    def init(self, n_neurons):\n",
        "        # Our Network\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(28*28, n_neurons),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_neurons, n_neurons),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_neurons, 10)\n",
        "        )\n",
        "        self.net.apply(self.init_weights)\n",
        "        # self.fc1 = nn.Linear(28*28, n_neurons)\n",
        "        # self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
        "        # self.fc3 = nn.Linear(n_neurons, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "        # x = self.fc1(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = self.fc2(x)\n",
        "        # x = F.relu(x)\n",
        "        # y = self.fc3(x)\n",
        "        # return y\n",
        "\n",
        "    def update_omega(self, W):\n",
        "        '''After completing training on a task, update the per-parameter regularization strength.\n",
        "\n",
        "        [W]         <dict> estimated parameter-specific contribution to changes in total loss of completed task\n",
        "        '''\n",
        "\n",
        "        # Loop over all parameters\n",
        "        for n, p in self.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                n = n.replace('.', '__')\n",
        "\n",
        "                # Find/calculate new values for quadratic penalty on parameters\n",
        "                p_prev = getattr(self, '{}_SI_prev_task'.format(n))\n",
        "                p_current = p.detach().clone()\n",
        "                p_change = p_current - p_prev\n",
        "                \n",
        "                omega_add = W[n]/(p_change**2 + self.epsilon)\n",
        "                try:\n",
        "                    omega = getattr(self, '{}_SI_omega'.format(n))\n",
        "                except AttributeError:\n",
        "                    omega = p.detach().clone().zero_()\n",
        "                omega_new = omega + omega_add\n",
        "\n",
        "                # Store these new values in the model\n",
        "                self.register_buffer('{}_SI_prev_task'.format(n), p_current)\n",
        "                self.register_buffer('{}_SI_omega'.format(n), omega_new)\n",
        "\n",
        "    def surrogate_loss(self):\n",
        "        '''Calculate SI's surrogate loss.'''\n",
        "        try:\n",
        "            losses = []\n",
        "            for n, p in self.named_parameters():\n",
        "                if p.requires_grad:\n",
        "                    # Retrieve previous parameter values and their normalized path integral (i.e., omega)\n",
        "                    n = n.replace('.', '__')\n",
        "                    prev_values = getattr(self, '{}_SI_prev_task'.format(n))\n",
        "                    omega = getattr(self, '{}_SI_omega'.format(n))\n",
        "                    # Calculate SI's surrogate loss, sum over all parameters\n",
        "                    losses.append((omega * (p-prev_values)**2).sum())\n",
        "            return sum(losses)\n",
        "        except AttributeError:\n",
        "            # SI-loss is 0 if there is no stored omega yet\n",
        "            return torch.tensor(0., device=self._device())\n",
        "\n",
        "    def _device(self):\n",
        "        return next(self.parameters()).device\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzvUNaI20C_G",
        "colab_type": "text"
      },
      "source": [
        "# Running our experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "846N7L_Q2asU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_task(model, device, train_loader, optimizer, batch_log = 0):\n",
        "    model.train()\n",
        "    # Prepare <dicts> to store running importance estimates and param-values before update\n",
        "    W = {}\n",
        "    param_old = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            name = name.replace('.', '__')\n",
        "            W[name] = param.data.clone().zero_()\n",
        "            param_old[name] = param.data.clone()\n",
        "\n",
        "    losses = []\n",
        "    total_losses = []\n",
        "    for k in range(n_epochs):\n",
        "        if print_messages:\n",
        "            print(\"----> Epoch {}:\".format(k))\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get the prediction\n",
        "            y_hat = model(x)\n",
        "\n",
        "            # Calculate training-precision\n",
        "            precision = (y == y_hat.max(1)[1]).sum().item() / x.size(0)\n",
        "\n",
        "            # Calculate the loss using cross entropy\n",
        "            # and the surrogate loss\n",
        "            loss = F.cross_entropy(input=y_hat, target=y, reduction='mean')\n",
        "            surrogate_loss = model.surrogate_loss()\n",
        "            total_loss = loss + model.si_c * surrogate_loss\n",
        "\n",
        "            # Backpropagate errors\n",
        "            total_loss.backward()\n",
        "\n",
        "            # Take optimization-step\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update running parameter importance estimates in W\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.requires_grad:\n",
        "                    name = name.replace('.', '__')\n",
        "                    if param.grad is not None:\n",
        "                        W[name].add_(-param.grad*(param.detach()-param_old[name]))\n",
        "                    param_old[name] = param.detach().clone()\n",
        "\n",
        "            # Print out a log\n",
        "            if batch_idx % batch_log == 0:\n",
        "                losses.append(loss.item())\n",
        "                total_losses.append(total_loss.item())\n",
        "                if print_messages:\n",
        "                    print('---->[{}/{} ({:.0f}%)]\\tPrecision: {:.6f}\\tLoss: {:.6f}\\tSurrogate Loss: {:.6f}\\tTotal Loss: {:.6f}'.format(\n",
        "                        batch_idx * len(x), len(train_loader.dataset),\n",
        "                        100. * batch_idx / len(train_loader), \n",
        "                        precision, loss.item(), surrogate_loss.item(), total_loss.item()))\n",
        "            \n",
        "    # After finishing training on a task, update the omega value in the model\n",
        "    model.update_omega(W)\n",
        "\n",
        "    return losses, total_losses\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            test_loss += F.cross_entropy(input=y_hat, target=y, reduction='mean')\n",
        "            pred = y_hat.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "    \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    return correct, test_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VznhdxdQapPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config={\n",
        "    \"lr\": 0.02, \n",
        "    \"si_c\": 0.9, \n",
        "    \"si_epsilon\": 0.01,\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"batch_size\": 64,\n",
        "    \"n_neurons\": 100,\n",
        "    \"momentum\": 0.4,\n",
        "    \"sample_size\": 20000\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpPc6HSRvHYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(config): \n",
        "    # Use cuda?\n",
        "    cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "    # Create Model\n",
        "    model = SIModel()\n",
        "    model.init(config[\"n_neurons\"])\n",
        "    model.to(device)\n",
        "    optim_list = [{'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': config['lr'], 'momentum': config['momentum']}]\n",
        "    if config['optimizer'] == \"adam\":\n",
        "        optimizer = optim.Adam(optim_list, betas=(0.9, 0.999))\n",
        "    else:\n",
        "        optimizer = optim.SGD(optim_list)\n",
        "\n",
        "    # SI Parameters\n",
        "    model.si_c = config[\"si_c\"]\n",
        "    model.epsilon = config[\"si_epsilon\"]\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            name = name.replace('.', '__')\n",
        "            model.register_buffer('{}_SI_prev_task'.format(name), param.data.clone())\n",
        "\n",
        "    # Load our test data\n",
        "    test_loaders = []\n",
        "    for i in range(n_tasks):\n",
        "        test_loaders.append(data_utils.DataLoader(data_utils.TensorDataset(tasks_te[i][1], tasks_te[i][2]), batch_size=1000, shuffle = False))\n",
        "\n",
        "    # Training\n",
        "    if print_messages:\n",
        "        print(\"--> Training:\")\n",
        "\n",
        "    total_acc = []\n",
        "    total_test_losses = []\n",
        "\n",
        "    for i in range(n_tasks):\n",
        "        if print_messages:\n",
        "            print(\"--> Training Task {}:\".format(i))\n",
        "\n",
        "        perm = np.random.permutation(tasks_tr[i][1].size(0))\n",
        "        perm = perm[:config['sample_size']]\n",
        "        train_data = data_utils.TensorDataset(tasks_tr[i][1], tasks_tr[i][2])\n",
        "        train_loader = data_utils.DataLoader(train_data, batch_size=config[\"batch_size\"], \n",
        "                                      sampler = data_utils.SubsetRandomSampler(perm), drop_last = True)\n",
        "        # train_loader = data_utils.DataLoader(train_data, batch_size=config[\"batch_size\"], \n",
        "        #                                      shuffle = True, drop_last = True)\n",
        "        \n",
        "        train_losses, total_train_losses = train_task(model, device, train_loader, optimizer, 1000)\n",
        "        \n",
        "        # Reset the optimizer (if using adam)\n",
        "        if config['optimizer'] == \"adam\":\n",
        "            model.optimizer = optim.Adam(optim_list, betas=(0.9, 0.999))\n",
        "\n",
        "        if print_messages:\n",
        "            print(train_losses)\n",
        "            print(total_train_losses)\n",
        "            print(\"--> Finished Training Task {}. Starting Test phase:\".format(i))\n",
        "\n",
        "        # Get our accuracy metrics on all test sets\n",
        "        acc = []\n",
        "        test_losses = []\n",
        "        for j in range(n_tasks):\n",
        "            correct, test_loss = test(model, device, test_loaders[j])\n",
        "            acc.append(100. * correct / len(test_loaders[j].dataset))\n",
        "            test_losses.append(test_loss)\n",
        "            if print_messages:\n",
        "                print('---->Test set {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "                    j, test_loss, correct, len(test_loaders[j].dataset),\n",
        "                    100. * correct / len(test_loaders[j].dataset)))\n",
        "        total_acc.append(acc)\n",
        "        total_test_losses.append(test_losses)\n",
        "    \n",
        "    # Get the accuracy metric as defined by Facebook paper: sum(R_Ti) \n",
        "    # where T is the test set of the last Task and i is the current trained task\n",
        "    return total_acc, total_test_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSffNskZDu8q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "12bebf5a-f0e7-4005-c8b5-82a8c1d6bd76"
      },
      "source": [
        "total_acc, total_test_losses = main(config)\n",
        "average_acc = np.mean(total_acc[n_tasks-1])\n",
        "print(\"Accuracy:\", average_acc)\n",
        "print(\"Confusion matrix:\")\n",
        "print('\\n'.join([','.join([str(item) for item in row]) for row in total_acc]))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 37.027\n",
            "Confusion matrix:\n",
            "93.5,89.8,72.96,64.45,51.03,28.24,24.0,13.58,11.06,9.54,9.65,10.16,11.21,14.64,16.55,19.04,19.43,19.46,21.14,20.1\n",
            "94.21,92.2,78.85,70.62,56.53,32.0,27.1,14.53,11.55,9.47,9.54,9.64,10.6,14.14,16.79,21.08,22.23,22.57,24.28,23.08\n",
            "94.12,92.05,79.3,71.56,57.71,33.36,28.03,14.98,11.77,9.64,9.96,9.87,10.78,14.45,16.96,21.05,22.53,22.72,24.82,23.37\n",
            "94.21,92.18,79.52,71.57,58.07,33.57,28.16,15.02,11.95,9.88,9.95,9.9,10.87,14.75,17.11,21.05,22.5,22.83,24.66,23.18\n",
            "94.19,92.08,79.55,71.72,58.31,33.81,28.47,14.99,11.86,9.8,9.91,10.0,10.93,14.76,17.08,20.97,22.33,22.66,24.52,23.06\n",
            "87.89,85.22,75.41,72.73,66.02,56.09,52.0,36.03,29.57,22.64,20.56,17.03,15.77,16.27,18.37,20.82,20.92,21.25,22.57,21.15\n",
            "86.54,83.93,74.86,72.41,66.48,57.89,54.43,38.7,32.56,24.8,22.54,17.9,16.37,16.26,18.23,20.38,20.26,20.97,22.42,20.92\n",
            "78.96,73.26,61.48,59.14,56.88,60.6,61.8,59.43,53.63,41.36,38.64,26.82,22.9,18.74,18.75,20.26,18.98,20.3,22.28,20.46\n",
            "78.36,72.74,60.57,58.4,56.4,60.14,61.55,59.66,54.33,42.26,39.88,27.13,22.74,18.7,18.56,20.13,18.79,20.04,22.19,20.5\n",
            "78.03,72.32,60.01,57.98,56.26,60.24,61.94,60.22,55.19,43.68,40.82,27.42,23.26,18.92,18.98,20.22,18.88,20.31,22.2,20.58\n",
            "77.87,72.4,59.9,57.75,55.25,58.39,59.47,59.12,54.96,44.95,42.77,29.43,24.72,20.24,20.28,20.54,19.18,20.22,21.91,20.39\n",
            "77.94,72.52,60.09,57.83,54.83,57.83,58.97,58.77,55.03,45.37,43.08,30.77,25.74,21.24,20.77,20.74,19.22,20.09,21.74,20.39\n",
            "77.6,72.1,59.71,57.43,54.47,57.35,58.61,58.54,55.14,45.37,43.18,31.32,26.37,21.78,21.28,20.79,19.22,20.05,21.66,20.3\n",
            "72.35,66.77,50.65,46.59,42.09,41.02,40.13,36.65,34.77,33.67,34.54,41.08,45.74,51.6,46.96,34.9,25.95,23.45,20.51,20.48\n",
            "70.47,65.15,49.64,46.1,41.98,41.66,41.0,37.49,35.36,34.99,35.66,42.8,47.19,52.3,48.77,35.59,26.36,23.66,20.38,20.15\n",
            "70.92,65.15,49.74,46.44,42.26,41.71,41.04,37.13,35.03,34.25,35.09,40.94,45.6,51.41,48.84,36.44,27.09,24.35,20.69,20.48\n",
            "56.75,51.43,38.28,35.12,30.52,26.43,27.42,27.89,26.23,22.08,24.88,31.03,33.83,44.66,45.96,49.09,48.76,45.76,37.88,33.74\n",
            "55.9,50.82,37.89,34.81,30.33,25.89,26.8,28.08,26.39,22.23,25.02,31.25,33.96,44.23,45.28,48.63,49.42,46.56,39.86,35.99\n",
            "55.97,51.01,38.18,34.96,30.39,25.88,26.81,27.89,26.37,22.32,24.97,31.36,33.79,44.07,45.17,48.82,49.52,46.72,39.94,36.04\n",
            "56.07,50.92,37.95,34.88,30.48,25.94,26.97,28.2,26.52,22.41,25.12,31.43,33.52,44.09,45.15,48.65,49.41,46.61,40.01,36.21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVib6Oe6Grtc",
        "colab_type": "text"
      },
      "source": [
        "# Tune Hyperparamters using Ax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8UnL5g8Ls8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "outputId": "d55b83a2-1ba8-4b41-f29f-72ed94dd3eb3"
      },
      "source": [
        "def tune(config, objective):\n",
        "    total_acc, total_loss = main(config)\n",
        "    if objective == \"accuracy\":\n",
        "        return np.mean(total_acc[n_tasks-1])\n",
        "    elif objective == \"loss\":\n",
        "        return np.mean(total_loss[n_tasks-1])\n",
        "    else:\n",
        "        return\n",
        "\n",
        "from ax import optimize\n",
        "best_parameters, values, experiment, model = optimize(\n",
        "    parameters=[\n",
        "        {\n",
        "            \"name\": \"lr\",\n",
        "            \"type\": \"range\",\n",
        "            \"bounds\": [1e-6, 0.4], \n",
        "            \"log_scale\": True,\n",
        "            \"value_type\": \"float\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"si_c\",\n",
        "            \"type\": \"range\",\n",
        "            \"bounds\": [0.01, 1.0],\n",
        "            \"value_type\": \"float\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"si_epsilon\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": 0.01,\n",
        "            \"value_type\": \"float\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"batch_size\",\n",
        "            \"type\": \"choice\",\n",
        "            \"values\": [10, 64],\n",
        "            \"value_type\": \"int\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"sample_size\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": 10000,\n",
        "            \"value_type\": \"int\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"n_neurons\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": 100,\n",
        "            \"value_type\": \"int\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"momentum\",\n",
        "            \"type\": \"range\",\n",
        "            \"bounds\": [0., 1.],\n",
        "            \"value_type\": \"float\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"optimizer\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": \"adam\",\n",
        "            \"value_type\": \"str\",\n",
        "        },\n",
        "    ],\n",
        "    evaluation_function=lambda p: tune(p, \"accuracy\"),\n",
        "    objective_name='accuracy',\n",
        ")\n",
        "print(best_parameters)\n",
        "print(values)\n",
        "    #evaluation_function=lambda p: np.mean(main(p)[n_tasks-1]),\n",
        "    #minimize=True,)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 04-04 22:38:33] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 8 arms, GPEI for subsequent arms], generated 0 arm(s) so far). Iterations after 8 will take longer to generate due to model-fitting.\n",
            "[INFO 04-04 22:38:33] ax.service.managed_loop: Started full optimization with 20 steps.\n",
            "[INFO 04-04 22:38:33] ax.service.managed_loop: Running optimization trial 1...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ERROR! Session/line number was not unique in database. History logging moved to new session 60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[INFO 04-04 22:40:09] ax.service.managed_loop: Running optimization trial 2...\n",
            "[INFO 04-04 22:45:52] ax.service.managed_loop: Running optimization trial 3...\n",
            "[INFO 04-04 22:47:27] ax.service.managed_loop: Running optimization trial 4...\n",
            "[INFO 04-04 22:49:02] ax.service.managed_loop: Running optimization trial 5...\n",
            "[INFO 04-04 22:54:45] ax.service.managed_loop: Running optimization trial 6...\n",
            "[INFO 04-04 22:56:21] ax.service.managed_loop: Running optimization trial 7...\n",
            "[INFO 04-04 23:02:05] ax.service.managed_loop: Running optimization trial 8...\n",
            "[INFO 04-04 23:07:45] ax.service.managed_loop: Running optimization trial 9...\n",
            "[INFO 04-04 23:09:20] ax.service.managed_loop: Running optimization trial 10...\n",
            "[INFO 04-04 23:10:55] ax.service.managed_loop: Running optimization trial 11...\n",
            "[INFO 04-04 23:12:31] ax.service.managed_loop: Running optimization trial 12...\n",
            "[INFO 04-04 23:14:06] ax.service.managed_loop: Running optimization trial 13...\n",
            "[INFO 04-04 23:15:42] ax.service.managed_loop: Running optimization trial 14...\n",
            "[INFO 04-04 23:17:19] ax.service.managed_loop: Running optimization trial 15...\n",
            "[INFO 04-04 23:18:54] ax.service.managed_loop: Running optimization trial 16...\n",
            "[INFO 04-04 23:20:30] ax.service.managed_loop: Running optimization trial 17...\n",
            "[INFO 04-04 23:22:07] ax.service.managed_loop: Running optimization trial 18...\n",
            "[INFO 04-04 23:23:43] ax.service.managed_loop: Running optimization trial 19...\n",
            "[INFO 04-04 23:25:19] ax.service.managed_loop: Running optimization trial 20...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'lr': 0.002161103326179421, 'si_c': 0.9999999999999994, 'momentum': 0.3974544156588803, 'batch_size': 64, 'si_epsilon': 0.01, 'sample_size': 10000, 'n_neurons': 100, 'optimizer': 'adam'}\n",
            "({'accuracy': 67.21499463717565}, {'accuracy': {'accuracy': 2.4184398641919375e-05}})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1ggRGCm1f7n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "60e87293-61cd-4588-ba2d-628c30837bc8"
      },
      "source": [
        "values"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'accuracy': 54.37497428669556},\n",
              " {'accuracy': {'accuracy': 2.8317326613905426e-05}})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBCyM3fKJcUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}