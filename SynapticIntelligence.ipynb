{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SynapticIntelligence.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x4JqoVPDvOVs"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dchu1/AI_P2_cl/blob/master/SynapticIntelligence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfQFMn10URTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_tasks = 20\n",
        "n_epochs = 3\n",
        "print_messages = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEXzD84NQHfr",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyJtcwJQKwOg",
        "colab_type": "code",
        "outputId": "820b6f24-be19-46c0-ffc2-7a3b46d8072d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        }
      },
      "source": [
        "pip install ax-platform"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ax-platform\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c3/e5/defa97540bf23447f15d142a644eed9a9d9fd1925cf1e3c4f47a49282ec0/ax_platform-0.1.9-py3-none-any.whl (499kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from ax-platform) (1.4.1)\n",
            "Collecting botorch==0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/e4/d696b12a84d505e9592fb6f8458a968b19efc22e30cc517dd2d2817e27e4/botorch-0.2.1-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 15.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from ax-platform) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from ax-platform) (2.11.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from ax-platform) (4.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from ax-platform) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->ax-platform) (1.18.2)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.6/dist-packages (from botorch==0.2.1->ax-platform) (1.4.0)\n",
            "Collecting gpytorch>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/5f/ce79e35c1a36deb25a0eac0f67bfe85fb8350eb8e19223950c3d615e5e9a/gpytorch-1.0.1.tar.gz (229kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->ax-platform) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->ax-platform) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->ax-platform) (1.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from plotly->ax-platform) (1.12.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->ax-platform) (1.3.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->ax-platform) (0.14.1)\n",
            "Building wheels for collected packages: gpytorch\n",
            "  Building wheel for gpytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpytorch: filename=gpytorch-1.0.1-py2.py3-none-any.whl size=390441 sha256=e680f242fca8fc58d6e46da845572fffc6096d6ff449b5c0253001c64e24568f\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/2f/7a/3328e5713d796daeec2ce8ded141d5f3837253fc3c2a5c62e0\n",
            "Successfully built gpytorch\n",
            "Installing collected packages: gpytorch, botorch, ax-platform\n",
            "Successfully installed ax-platform-0.1.9 botorch-0.2.1 gpytorch-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdLhz_0GjWA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torch.utils.data as data_utils\n",
        "import numpy as np\n",
        "import subprocess\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn import init\n",
        "from torch.nn import Module\n",
        "from torch.nn import init\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "from IPython.core.debugger import set_trace\n",
        "\n",
        "n_tasks = 20\n",
        "n_epochs = 3\n",
        "print_messages = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4JqoVPDvOVs",
        "colab_type": "text"
      },
      "source": [
        "# Constructing Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrev6f7FvRIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_dataset(d, rotation):\n",
        "  result = torch.FloatTensor(d.size(0), 784)\n",
        "  tensor = transforms.ToTensor()\n",
        "\n",
        "  for i in range(d.size(0)):\n",
        "    img = Image.fromarray(d[i].numpy(), mode=\"L\")\n",
        "    result[i] = tensor(img.rotate(rotation)).view(784)\n",
        "  return result\n",
        "\n",
        "mnist_path = \"mnist.npz\"\n",
        "if not os.path.exists(os.path.join(\"/content\", mnist_path)):\n",
        "  subprocess.call(\"wget https://s3.amazonaws.com/img-datasets/mnist.npz\", shell=True)\n",
        "\n",
        "f = np.load(mnist_path)\n",
        "x_tr = torch.from_numpy(f[\"x_train\"])\n",
        "y_tr = torch.from_numpy(f[\"y_train\"]).long()\n",
        "x_te = torch.from_numpy(f[\"x_test\"])\n",
        "y_te = torch.from_numpy(f[\"y_test\"]).long()\n",
        "f.close()\n",
        "\n",
        "# Rotate Dataset\n",
        "tasks_tr = []\n",
        "tasks_te = []\n",
        "mnist_rot_path = \"mnist_rotations.pt\"\n",
        "if not os.path.exists(os.path.join(\"/content\", mnist_rot_path)):\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    for t in range(n_tasks):\n",
        "      min_rot = 1.0 * t / n_tasks * (180.0 - 0.0) + 0.0\n",
        "      max_rot = 1.0 * (t + 1) / n_tasks * (180.0 - 0.0) + 0.0\n",
        "      rot = random.random() * (max_rot - min_rot) + min_rot\n",
        "\n",
        "      tasks_tr.append([rot, rotate_dataset(x_tr, rot), y_tr])\n",
        "      tasks_te.append([rot, rotate_dataset(x_te, rot), y_te])\n",
        "\n",
        "    torch.save([tasks_tr, tasks_te], 'mnist_rotations.pt')\n",
        "else:\n",
        "    tasks_tr, tasks_te = torch.load('/content/mnist_rotations.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3w5psinhrGb",
        "colab_type": "text"
      },
      "source": [
        "# Defining Synaptic Intelligence Model (Simple)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DhQQzqOhuJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SIModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SIModel, self).__init__()\n",
        "\n",
        "        # SI Hyperparameters\n",
        "        self.si_c = 0.           #-> hyperparam: how strong to weigh SI-loss (\"regularisation strength\")\n",
        "        self.epsilon = 0.1      #-> dampening parameter: bounds 'omega' when squared parameter-change goes to 0\n",
        "    \n",
        "    def init_weights(self, m):\n",
        "        if type(m) == nn.Linear:\n",
        "            torch.nn.init.kaiming_uniform_(m.weight)\n",
        "            m.bias.data.fill_(0.01)\n",
        "\n",
        "    def init(self, n_neurons):\n",
        "        # Our Network\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(28*28, n_neurons),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_neurons, n_neurons),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_neurons, 10)\n",
        "        )\n",
        "        self.net.apply(self.init_weights)\n",
        "        # self.fc1 = nn.Linear(28*28, n_neurons)\n",
        "        # self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
        "        # self.fc3 = nn.Linear(n_neurons, 10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "        # x = self.fc1(x)\n",
        "        # x = F.relu(x)\n",
        "        # x = self.fc2(x)\n",
        "        # x = F.relu(x)\n",
        "        # y = self.fc3(x)\n",
        "        # return y\n",
        "\n",
        "    def update_omega(self, W):\n",
        "        '''After completing training on a task, update the per-parameter regularization strength.\n",
        "\n",
        "        [W]         <dict> estimated parameter-specific contribution to changes in total loss of completed task\n",
        "        '''\n",
        "\n",
        "        # Loop over all parameters\n",
        "        for n, p in self.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                n = n.replace('.', '__')\n",
        "\n",
        "                # Find/calculate new values for quadratic penalty on parameters\n",
        "                p_prev = getattr(self, '{}_SI_prev_task'.format(n))\n",
        "                p_current = p.detach().clone()\n",
        "                p_change = p_current - p_prev\n",
        "                \n",
        "                omega_add = W[n]/(p_change**2 + self.epsilon)\n",
        "                try:\n",
        "                    omega = getattr(self, '{}_SI_omega'.format(n))\n",
        "                except AttributeError:\n",
        "                    omega = p.detach().clone().zero_()\n",
        "                omega_new = omega + omega_add\n",
        "\n",
        "                # Store these new values in the model\n",
        "                self.register_buffer('{}_SI_prev_task'.format(n), p_current)\n",
        "                self.register_buffer('{}_SI_omega'.format(n), omega_new)\n",
        "\n",
        "    def surrogate_loss(self):\n",
        "        '''Calculate SI's surrogate loss.'''\n",
        "        try:\n",
        "            losses = []\n",
        "            for n, p in self.named_parameters():\n",
        "                if p.requires_grad:\n",
        "                    # Retrieve previous parameter values and their normalized path integral (i.e., omega)\n",
        "                    n = n.replace('.', '__')\n",
        "                    prev_values = getattr(self, '{}_SI_prev_task'.format(n))\n",
        "                    omega = getattr(self, '{}_SI_omega'.format(n))\n",
        "                    # Calculate SI's surrogate loss, sum over all parameters\n",
        "                    losses.append((omega * (p-prev_values)**2).sum())\n",
        "            return sum(losses)\n",
        "        except AttributeError:\n",
        "            # SI-loss is 0 if there is no stored omega yet\n",
        "            return torch.tensor(0., device=self._device())\n",
        "\n",
        "    def _device(self):\n",
        "        return next(self.parameters()).device\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzvUNaI20C_G",
        "colab_type": "text"
      },
      "source": [
        "# Running our experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "846N7L_Q2asU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_task(model, device, train_loader, optimizer, batch_log = 0):\n",
        "    model.train()\n",
        "    # Prepare <dicts> to store running importance estimates and param-values before update\n",
        "    W = {}\n",
        "    param_old = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            name = name.replace('.', '__')\n",
        "            W[name] = param.data.clone().zero_()\n",
        "            param_old[name] = param.data.clone()\n",
        "\n",
        "    losses = []\n",
        "    total_losses = []\n",
        "    for k in range(n_epochs):\n",
        "        if print_messages:\n",
        "            print(\"----> Epoch {}:\".format(k))\n",
        "        for batch_idx, (x, y) in enumerate(train_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Get the prediction\n",
        "            y_hat = model(x)\n",
        "\n",
        "            # Calculate training-precision\n",
        "            precision = (y == y_hat.max(1)[1]).sum().item() / x.size(0)\n",
        "\n",
        "            # Calculate the loss using cross entropy\n",
        "            # and the surrogate loss\n",
        "            loss = F.cross_entropy(input=y_hat, target=y, reduction='mean')\n",
        "            surrogate_loss = model.surrogate_loss()\n",
        "            total_loss = loss + model.si_c * surrogate_loss\n",
        "\n",
        "            # Backpropagate errors\n",
        "            total_loss.backward()\n",
        "\n",
        "            # Take optimization-step\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update running parameter importance estimates in W\n",
        "            # \"In practice, we can approximate w as the running sum of the \n",
        "            # product of the gradient g(w) and the parameter update\" \n",
        "            for name, param in model.named_parameters():\n",
        "                if param.requires_grad:\n",
        "                    name = name.replace('.', '__')\n",
        "                    if param.grad is not None:\n",
        "                        W[name].add_(-param.grad*(param.detach()-param_old[name]))\n",
        "                    param_old[name] = param.detach().clone()\n",
        "\n",
        "            # Print out a log\n",
        "            if batch_idx % batch_log == 0:\n",
        "                losses.append(loss.item())\n",
        "                total_losses.append(total_loss.item())\n",
        "                if print_messages:\n",
        "                    print('---->[{}/{} ({:.0f}%)]\\tPrecision: {:.6f}\\tLoss: {:.6f}\\tSurrogate Loss: {:.6f}\\tTotal Loss: {:.6f}'.format(\n",
        "                        batch_idx * len(x), len(train_loader.dataset),\n",
        "                        100. * batch_idx / len(train_loader), \n",
        "                        precision, loss.item(), surrogate_loss.item(), total_loss.item()))\n",
        "            \n",
        "    # After finishing training on a task, update the omega value in the model\n",
        "    model.update_omega(W)\n",
        "\n",
        "    return losses, total_losses\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in test_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_hat = model(x)\n",
        "            test_loss += F.cross_entropy(input=y_hat, target=y, reduction='mean')\n",
        "            pred = y_hat.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(y.view_as(pred)).sum().item()\n",
        "    \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    return correct, test_loss\n",
        "\n",
        "def eval_on_tasks(model, device, test_loaders):\n",
        "    acc = []\n",
        "    test_losses = []\n",
        "    for j in range(n_tasks):\n",
        "        correct, test_loss = test(model, device, test_loaders[j])\n",
        "        acc.append(100. * correct / len(test_loaders[j].dataset))\n",
        "        test_losses.append(test_loss)\n",
        "        if print_messages:\n",
        "            print('---->Test set {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "                j, test_loss, correct, len(test_loaders[j].dataset),\n",
        "                100. * correct / len(test_loaders[j].dataset)))\n",
        "    return acc, test_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VznhdxdQapPC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config={\n",
        "    \"lr\": 0.003, \n",
        "    \"si_c\": 0.152, \n",
        "    \"si_epsilon\": 0.01,\n",
        "    \"optimizer\": \"adam\",\n",
        "    \"batch_size\": 64,\n",
        "    \"n_neurons\": 100,\n",
        "    \"sample_size\": 60000\n",
        "    }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpPc6HSRvHYK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(config): \n",
        "    # Use cuda?\n",
        "    cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "\n",
        "    # Create Model\n",
        "    model = SIModel()\n",
        "    model.init(config[\"n_neurons\"])\n",
        "    model.to(device)\n",
        "    optim_list = [{'params': filter(lambda p: p.requires_grad, model.parameters()), 'lr': config['lr']}]\n",
        "    if config['optimizer'] == \"adam\":\n",
        "        optimizer = optim.Adam(optim_list, betas=(0.9, 0.999))\n",
        "    else:\n",
        "        optimizer = optim.SGD(optim_list)\n",
        "\n",
        "    # SI Parameters\n",
        "    model.si_c = config[\"si_c\"]\n",
        "    model.epsilon = config[\"si_epsilon\"]\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            name = name.replace('.', '__')\n",
        "            model.register_buffer('{}_SI_prev_task'.format(name), param.data.clone())\n",
        "\n",
        "    # Load our test data\n",
        "    test_loaders = []\n",
        "    for i in range(n_tasks):\n",
        "        test_loaders.append(data_utils.DataLoader(data_utils.TensorDataset(tasks_te[i][1], tasks_te[i][2]), batch_size=1000, shuffle = False))\n",
        "\n",
        "    # Training\n",
        "    if print_messages:\n",
        "        print(\"--> Training:\")\n",
        "\n",
        "    total_acc = []\n",
        "    total_test_losses = []\n",
        "    \n",
        "    # Before we start training we will get a baseline by evaluating our tasks\n",
        "    acc, test_losses = eval_on_tasks(model, device, test_loaders)\n",
        "    total_acc.append(acc)\n",
        "    total_test_losses.append(test_losses)\n",
        "\n",
        "    for i in range(n_tasks):\n",
        "        if print_messages:\n",
        "            print(\"--> Training Task {}:\".format(i))\n",
        "\n",
        "        perm = np.random.permutation(tasks_tr[i][1].size(0))\n",
        "        perm = perm[:config['sample_size']]\n",
        "        train_data = data_utils.TensorDataset(tasks_tr[i][1], tasks_tr[i][2])\n",
        "        train_loader = data_utils.DataLoader(train_data, batch_size=config[\"batch_size\"], \n",
        "                                      sampler = data_utils.SubsetRandomSampler(perm), drop_last = True)\n",
        "        # train_loader = data_utils.DataLoader(train_data, batch_size=config[\"batch_size\"], \n",
        "        #                                      shuffle = True, drop_last = True)\n",
        "        \n",
        "        train_losses, total_train_losses = train_task(model, device, train_loader, optimizer, 100)\n",
        "        \n",
        "        # Reset the optimizer (if using adam)\n",
        "        if config['optimizer'] == \"adam\":\n",
        "            model.optimizer = optim.Adam(optim_list, betas=(0.9, 0.999))\n",
        "\n",
        "        if print_messages:\n",
        "            print(train_losses)\n",
        "            print(total_train_losses)\n",
        "            print(\"--> Finished Training Task {}. Starting Test phase:\".format(i))\n",
        "\n",
        "        # Get our accuracy metrics on all test sets\n",
        "        # acc = []\n",
        "        # test_losses = []\n",
        "        # for j in range(n_tasks):\n",
        "        #     correct, test_loss = test(model, device, test_loaders[j])\n",
        "        #     acc.append(100. * correct / len(test_loaders[j].dataset))\n",
        "        #     test_losses.append(test_loss)\n",
        "        #     if print_messages:\n",
        "        #         print('---->Test set {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\n",
        "        #             j, test_loss, correct, len(test_loaders[j].dataset),\n",
        "        #             100. * correct / len(test_loaders[j].dataset)))\n",
        "        acc, test_losses = eval_on_tasks(model, device, test_loaders)\n",
        "        total_acc.append(acc)\n",
        "        total_test_losses.append(test_losses)\n",
        "    \n",
        "    return total_acc, total_test_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSffNskZDu8q",
        "colab_type": "code",
        "outputId": "f69f15b6-d6d9-4a4d-8821-d75a77621d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "total_acc, total_test_losses = main(config)\n",
        "\n",
        "# Get the accuracy metric as defined by Facebook paper: sum(R_Ti) \n",
        "# where T is the test set of the last Task and i is the current trained task\n",
        "average_acc = np.mean(total_acc[n_tasks-1])\n",
        "print(\"Accuracy:\", average_acc)\n",
        "print(\"Confusion matrix:\")\n",
        "print('\\n'.join([','.join([str(item) for item in row]) for row in total_acc]))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 58.57899999999999\n",
            "Confusion matrix:\n",
            "12.46,13.32,13.42,11.23,12.26,13.76,13.83,13.41,11.72,9.1,9.02,10.44,10.15,10.06,11.12,10.45,10.26,9.74,8.68,8.73\n",
            "97.02,91.66,90.99,77.71,64.69,38.91,30.18,17.74,13.59,11.06,11.46,10.93,13.55,13.83,18.22,21.61,24.21,25.25,26.3,27.73\n",
            "95.07,95.9,95.67,89.43,81.09,54.05,42.73,24.81,19.4,15.1,13.3,12.02,11.42,11.24,14.24,18.85,22.66,25.54,27.37,29.33\n",
            "95.39,95.8,95.61,90.81,83.16,55.96,43.44,25.85,20.0,15.65,13.88,12.51,12.45,12.17,16.0,21.23,25.25,27.56,28.96,30.67\n",
            "92.93,95.71,95.68,92.9,87.94,63.83,51.01,30.51,23.44,17.28,15.48,13.61,12.63,12.46,15.79,20.42,24.2,26.91,28.18,30.38\n",
            "88.18,93.68,93.93,93.84,92.1,78.17,68.8,47.21,35.42,25.49,19.51,15.91,12.33,12.16,13.54,18.49,21.92,24.76,27.03,29.61\n",
            "82.65,89.79,90.58,92.93,92.95,86.71,81.05,63.8,50.32,36.99,26.43,20.46,13.29,12.92,12.75,15.97,18.86,21.57,23.9,27.27\n",
            "77.33,86.66,87.13,90.71,91.14,89.82,87.19,76.18,64.11,48.4,33.53,27.27,17.29,16.79,13.75,14.6,15.72,17.59,20.35,23.65\n",
            "71.84,82.49,83.16,87.61,89.1,90.03,88.56,81.21,71.45,57.14,40.87,32.74,19.72,19.25,15.18,14.27,14.76,15.31,17.72,21.47\n",
            "65.77,77.54,77.65,83.34,85.49,88.96,88.55,84.84,78.47,63.99,47.25,37.01,22.93,22.2,17.2,16.03,15.78,15.61,17.47,21.11\n",
            "64.36,76.68,76.76,82.64,84.44,87.9,88.02,85.51,80.87,70.82,57.41,46.99,29.48,28.07,18.92,16.26,15.0,14.55,16.34,20.17\n",
            "63.58,74.65,74.86,80.79,83.62,87.42,87.72,86.25,83.04,75.72,65.39,56.41,35.77,34.25,21.82,17.35,15.14,14.73,16.24,20.42\n",
            "63.06,72.96,73.09,78.85,81.93,85.4,85.88,84.84,82.52,77.86,70.49,63.5,42.93,40.24,25.65,19.65,16.07,15.1,16.43,19.65\n",
            "63.63,71.02,71.28,77.01,79.73,83.83,83.58,83.42,81.72,78.9,73.67,68.54,54.38,52.22,37.2,30.75,26.23,22.7,22.52,22.02\n",
            "62.51,69.8,70.4,76.25,78.76,82.88,82.75,82.66,80.78,78.45,74.81,70.34,58.8,56.97,40.58,31.54,25.81,20.74,19.85,19.17\n",
            "66.67,71.24,72.2,77.4,78.94,81.94,81.59,80.97,79.57,76.99,74.39,70.53,62.16,60.83,47.87,38.99,31.7,26.41,24.21,20.77\n",
            "66.87,70.96,71.53,77.03,78.34,80.32,79.87,78.76,77.36,75.0,72.33,69.36,63.66,62.39,52.67,46.15,38.15,32.21,27.98,22.54\n",
            "66.51,70.38,70.95,76.2,77.81,79.66,79.44,78.39,76.89,74.62,72.21,69.57,65.0,64.2,57.43,53.64,47.38,41.36,35.51,26.35\n",
            "62.15,62.1,62.03,61.53,60.33,61.85,61.87,60.43,59.63,58.3,55.19,54.82,54.86,55.73,59.11,64.41,68.05,66.66,59.46,46.12\n",
            "61.38,62.27,62.08,60.78,59.19,58.77,57.79,55.51,54.71,54.25,50.84,50.68,51.17,52.07,58.36,65.07,69.64,70.48,65.25,51.29\n",
            "57.76,60.13,59.73,58.42,56.51,55.75,54.61,54.02,52.94,52.53,49.68,49.27,49.64,50.93,56.74,63.28,68.46,70.99,68.83,59.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVib6Oe6Grtc",
        "colab_type": "text"
      },
      "source": [
        "# Tune Hyperparamters using Ax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8UnL5g8Ls8B",
        "colab_type": "code",
        "outputId": "dddedbc3-2b5b-44fb-b719-b0dee402325c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "def tune(config, objective):\n",
        "    total_acc, total_loss = main(config)\n",
        "    if objective == \"accuracy\":\n",
        "        return np.mean(total_acc[n_tasks-1])\n",
        "    elif objective == \"loss\":\n",
        "        return np.mean(total_loss[n_tasks-1])\n",
        "    else:\n",
        "        return\n",
        "\n",
        "from ax import optimize\n",
        "best_parameters, values, experiment, model = optimize(\n",
        "    parameters=[\n",
        "        {\n",
        "            \"name\": \"lr\",\n",
        "            \"type\": \"range\",\n",
        "            \"bounds\": [1e-4, 0.4], \n",
        "            \"log_scale\": True,\n",
        "            \"value_type\": \"float\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"si_c\",\n",
        "            \"type\": \"range\",\n",
        "            \"bounds\": [0.01, 0.5],\n",
        "            \"value_type\": \"float\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"si_epsilon\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": 0.01,\n",
        "            \"value_type\": \"float\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"batch_size\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": 64,\n",
        "            \"value_type\": \"int\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"sample_size\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": 10000,\n",
        "            \"value_type\": \"int\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"n_neurons\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": 100,\n",
        "            \"value_type\": \"int\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"optimizer\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": \"adam\",\n",
        "            \"value_type\": \"str\",\n",
        "        },\n",
        "    ],\n",
        "    evaluation_function=lambda p: tune(p, \"accuracy\"),\n",
        "    objective_name='accuracy',\n",
        ")\n",
        "print(best_parameters)\n",
        "print(values)\n",
        "    #evaluation_function=lambda p: np.mean(main(p)[n_tasks-1]),\n",
        "    #minimize=True,)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 04-05 02:39:10] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 8 arms, GPEI for subsequent arms], generated 0 arm(s) so far). Iterations after 8 will take longer to generate due to model-fitting.\n",
            "[INFO 04-05 02:39:10] ax.service.managed_loop: Started full optimization with 20 steps.\n",
            "[INFO 04-05 02:39:10] ax.service.managed_loop: Running optimization trial 1...\n",
            "[INFO 04-05 02:40:23] ax.service.managed_loop: Running optimization trial 2...\n",
            "[INFO 04-05 02:41:35] ax.service.managed_loop: Running optimization trial 3...\n",
            "[INFO 04-05 02:42:47] ax.service.managed_loop: Running optimization trial 4...\n",
            "[INFO 04-05 02:43:59] ax.service.managed_loop: Running optimization trial 5...\n",
            "[INFO 04-05 02:45:11] ax.service.managed_loop: Running optimization trial 6...\n",
            "[INFO 04-05 02:46:23] ax.service.managed_loop: Running optimization trial 7...\n",
            "[INFO 04-05 02:47:34] ax.service.managed_loop: Running optimization trial 8...\n",
            "[INFO 04-05 02:48:46] ax.service.managed_loop: Running optimization trial 9...\n",
            "[INFO 04-05 02:49:59] ax.service.managed_loop: Running optimization trial 10...\n",
            "[INFO 04-05 02:51:12] ax.service.managed_loop: Running optimization trial 11...\n",
            "[INFO 04-05 02:52:25] ax.service.managed_loop: Running optimization trial 12...\n",
            "[INFO 04-05 02:53:38] ax.service.managed_loop: Running optimization trial 13...\n",
            "[INFO 04-05 02:54:51] ax.service.managed_loop: Running optimization trial 14...\n",
            "[INFO 04-05 02:56:04] ax.service.managed_loop: Running optimization trial 15...\n",
            "[INFO 04-05 02:57:16] ax.service.managed_loop: Running optimization trial 16...\n",
            "[INFO 04-05 02:58:28] ax.service.managed_loop: Running optimization trial 17...\n",
            "[INFO 04-05 02:59:44] ax.service.managed_loop: Running optimization trial 18...\n",
            "[INFO 04-05 03:00:56] ax.service.managed_loop: Running optimization trial 19...\n",
            "[INFO 04-05 03:02:09] ax.service.managed_loop: Running optimization trial 20...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'lr': 0.0029937139316947364, 'si_c': 0.15201809808880937, 'si_epsilon': 0.01, 'batch_size': 64, 'sample_size': 10000, 'n_neurons': 100, 'momentum': 0.9, 'optimizer': 'adam'}\n",
            "({'accuracy': 58.60698750153793}, {'accuracy': {'accuracy': 2.0230659953751954e-05}})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VY2BX7OXi0pF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "outputId": "2d476a56-0af2-4c65-9d8a-d8474c6160b9"
      },
      "source": [
        "best_parameters, values, experiment, model = optimize(\n",
        "    parameters=[\n",
        "        {\n",
        "            \"name\": \"lr\",\n",
        "            \"type\": \"range\",\n",
        "            \"bounds\": [1e-4, 0.4], \n",
        "            \"log_scale\": True,\n",
        "            \"value_type\": \"float\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"si_c\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": 0.152,\n",
        "            \"value_type\": \"float\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"si_epsilon\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": 0.01,\n",
        "            \"value_type\": \"float\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"batch_size\",\n",
        "            \"type\": \"choice\",\n",
        "            \"values\": [64, 128, 256],\n",
        "            \"value_type\": \"int\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"sample_size\",\n",
        "            \"type\": \"choice\",\n",
        "            \"values\": [1000, 5000, 10000, 20000, 40000, 60000],\n",
        "            \"value_type\": \"int\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"n_neurons\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": 100,\n",
        "            \"value_type\": \"int\",\n",
        "        },\n",
        "        {  \n",
        "            \"name\": \"optimizer\",\n",
        "            \"type\": \"fixed\",\n",
        "            \"value\": \"adam\",\n",
        "            \"value_type\": \"str\",\n",
        "        },\n",
        "    ],\n",
        "    evaluation_function=lambda p: tune(p, \"accuracy\"),\n",
        "    objective_name='accuracy',\n",
        ")\n",
        "print(best_parameters)\n",
        "print(values)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO 04-05 04:20:04] ax.modelbridge.dispatch_utils: Using Sobol generation strategy.\n",
            "[INFO 04-05 04:20:04] ax.service.managed_loop: Started full optimization with 20 steps.\n",
            "[INFO 04-05 04:20:04] ax.service.managed_loop: Running optimization trial 1...\n",
            "[INFO 04-05 04:20:43] ax.service.managed_loop: Running optimization trial 2...\n",
            "[INFO 04-05 04:21:30] ax.service.managed_loop: Running optimization trial 3...\n",
            "[INFO 04-05 04:22:17] ax.service.managed_loop: Running optimization trial 4...\n",
            "[INFO 04-05 04:25:00] ax.service.managed_loop: Running optimization trial 5...\n",
            "[INFO 04-05 04:26:03] ax.service.managed_loop: Running optimization trial 6...\n",
            "[INFO 04-05 04:27:13] ax.service.managed_loop: Running optimization trial 7...\n",
            "[INFO 04-05 04:29:13] ax.service.managed_loop: Running optimization trial 8...\n",
            "[INFO 04-05 04:30:30] ax.service.managed_loop: Running optimization trial 9...\n",
            "[INFO 04-05 04:31:05] ax.service.managed_loop: Running optimization trial 10...\n",
            "[INFO 04-05 04:35:11] ax.service.managed_loop: Running optimization trial 11...\n",
            "[INFO 04-05 04:35:57] ax.service.managed_loop: Running optimization trial 12...\n",
            "[INFO 04-05 04:38:54] ax.service.managed_loop: Running optimization trial 13...\n",
            "[INFO 04-05 04:43:01] ax.service.managed_loop: Running optimization trial 14...\n",
            "[INFO 04-05 04:45:00] ax.service.managed_loop: Running optimization trial 15...\n",
            "[INFO 04-05 04:46:16] ax.service.managed_loop: Running optimization trial 16...\n",
            "[INFO 04-05 04:47:04] ax.service.managed_loop: Running optimization trial 17...\n",
            "[INFO 04-05 04:48:15] ax.service.managed_loop: Running optimization trial 18...\n",
            "[INFO 04-05 04:51:09] ax.service.managed_loop: Running optimization trial 19...\n",
            "[INFO 04-05 04:53:08] ax.service.managed_loop: Running optimization trial 20...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "{'lr': 0.0017180968355585689, 'batch_size': 64, 'sample_size': 60000, 'si_c': 0.152, 'si_epsilon': 0.01, 'n_neurons': 100, 'momentum': 0.9, 'optimizer': 'adam'}\n",
            "({'accuracy': 59.4385}, {'accuracy': {'accuracy': 0.0}})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykfkE8f3kR_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}